{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# DL Practical 10\n> 19BCE245 - Aayush Shah\n- GAN for MNIST like image generation","metadata":{}},{"cell_type":"code","source":"from tensorflow import keras\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom keras.layers import Input\nfrom keras.models import Model, Sequential\nfrom keras.layers.core import Reshape, Dense, Dropout, Flatten\nfrom keras.layers.advanced_activations import LeakyReLU\nfrom keras.layers.convolutional import Convolution2D, UpSampling2D\n# from keras.layers.normalization import BatchNormalization\nfrom tensorflow.keras.layers import BatchNormalization\nfrom keras.datasets import mnist\nfrom tensorflow.keras.optimizers import Adam\nfrom keras import initializers\n\n# Deterministic output.\n# Tired of seeing the same results every time? Remove the line below.\n#np.random.seed(1000)\n\n# The results are a little better when the dimensionality of the random vector is only 10.\n# The dimensionality has been left at 100 for consistency with other GAN implementations.\nrandomDim = 100\n\n# Load MNIST data\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\nX_train = (X_train.astype(np.float32) - 127.5)/127.5\nX_train = X_train.reshape(60000, 784)\n\n# Optimizer\nadam = Adam(learning_rate=0.0002, beta_1=0.5)\n\ngenerator = Sequential()\ngenerator.add(Dense(256, input_dim=randomDim, kernel_initializer=initializers.RandomNormal(stddev=0.02)))\ngenerator.add(LeakyReLU(0.2))\ngenerator.add(Dense(512))\ngenerator.add(LeakyReLU(0.2))\ngenerator.add(Dense(1024))\ngenerator.add(LeakyReLU(0.2))\ngenerator.add(Dense(784, activation='tanh'))\n#generator.compile(loss='binary_crossentropy', optimizer=adam)\n\ndiscriminator = Sequential()\ndiscriminator.add(Dense(1024, input_dim=784, kernel_initializer=initializers.RandomNormal(stddev=0.02)))\ndiscriminator.add(LeakyReLU(0.2))\ndiscriminator.add(Dropout(0.3))\ndiscriminator.add(Dense(512))\ndiscriminator.add(LeakyReLU(0.2))\ndiscriminator.add(Dropout(0.3))\ndiscriminator.add(Dense(256))\ndiscriminator.add(LeakyReLU(0.2))\ndiscriminator.add(Dropout(0.3))\ndiscriminator.add(Dense(1, activation='sigmoid'))\ndiscriminator.compile(loss='binary_crossentropy', optimizer=adam)\n\n# Combined network\ndiscriminator.trainable = False\nganInput = Input(shape=(randomDim,))\nx = generator(ganInput)\nganOutput = discriminator(x)\ngan = Model(inputs=ganInput, outputs=ganOutput)\ngan.compile(loss='binary_crossentropy', optimizer=adam)\n\ndLosses = []\ngLosses = []\n\n# Plot the loss from each batch\ndef plotLoss(epoch):\n    plt.figure(figsize=(10, 8))\n    plt.plot(dLosses, label='Discriminitive loss')\n    plt.plot(gLosses, label='Generative loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    #plt.savefig('images/gan_loss_epoch_%d.png' % epoch)\n\n# Create a wall of generated MNIST images\ndef plotGeneratedImages(epoch, examples=100, dim=(10, 10), figsize=(10, 10)):\n    noise = np.random.normal(0, 1, size=[examples, randomDim])\n    generatedImages = generator.predict(noise)\n    generatedImages = generatedImages.reshape(examples, 28, 28)\n\n    plt.figure(figsize=figsize)\n    for i in range(generatedImages.shape[0]):\n        plt.subplot(dim[0], dim[1], i+1)\n        plt.imshow(generatedImages[i], interpolation='nearest', cmap='gray_r')\n        plt.axis('off')\n    plt.tight_layout()\n    #plt.savefig('images/gan_generated_image_epoch_%d.png' % epoch)\n\n# Save the generator and discriminator networks (and weights) for later use\ndef saveModels(epoch):\n    generator.save('models/gan_generator_epoch_%d.h5' % epoch)\n    discriminator.save('models/gan_discriminator_epoch_%d.h5' % epoch)\n\ndef train(epochs=1, batchSize=128):\n    batchCount = X_train.shape[0] / batchSize\n    print('Epochs:', epochs, sep='')\n    print( 'Batch size:', batchSize)\n    print ('Batches per epoch:', batchCount)\n    #print()\n\n    for e in range(1, epochs+1):\n        print ('-'*15, 'Epoch %d' % e, '-'*15)\n        for mb in (range(int(batchCount))):\n            # Get a random set of input noise and images\n            noise = np.random.normal(0, 1, size=[batchSize, randomDim])\n            imageBatch = X_train[np.random.randint(0, X_train.shape[0], size=batchSize)]\n\n            # Generate fake MNIST images\n            generatedImages = generator.predict(noise)\n            # print np.shape(imageBatch), np.shape(generatedImages)\n            X = np.concatenate([imageBatch, generatedImages])\n\n            # Labels for generated and real data\n            yDis = np.zeros(2*batchSize)\n            # One-sided label smoothing\n            yDis[:batchSize] = 0.9\n\n            # Train discriminator\n            discriminator.trainable = True\n            dloss = discriminator.train_on_batch(X, yDis)\n\n            # Train generator\n            noise = np.random.normal(0, 1, size=[batchSize, randomDim])\n            yGen = np.ones(batchSize)\n            discriminator.trainable = False\n            gloss = gan.train_on_batch(noise, yGen)\n\n        # Store loss of most recent batch from this epoch\n        dLosses.append(dloss)\n        gLosses.append(gloss)\n\n        if e == 1 or e % 20 == 0:\n            plotGeneratedImages(e)\n            #saveModels(e)\n\n    # Plot losses from every epoch\n    plotLoss(e)\n\nif __name__ == '__main__':\n    train(20, 128)","metadata":{"id":"KoGBMEtJX-9M","execution":{"iopub.status.busy":"2022-05-06T14:07:25.233131Z","iopub.execute_input":"2022-05-06T14:07:25.233437Z"},"trusted":true},"execution_count":null,"outputs":[]}]}