{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport tensorflow as tf\nfrom keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D, Flatten, BatchNormalization, Dropout\nfrom keras.models import Model, Sequential\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.layers.advanced_activations import LeakyReLU\nfrom tensorflow.keras.optimizers import Adam\nfrom keras.datasets import mnist\nfrom keras import initializers\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2022-05-09T18:52:29.979843Z","iopub.execute_input":"2022-05-09T18:52:29.980135Z","iopub.status.idle":"2022-05-09T18:52:29.986916Z","shell.execute_reply.started":"2022-05-09T18:52:29.980104Z","shell.execute_reply":"2022-05-09T18:52:29.986000Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"randomDim = 100","metadata":{"execution":{"iopub.status.busy":"2022-05-09T18:52:50.289714Z","iopub.execute_input":"2022-05-09T18:52:50.290364Z","iopub.status.idle":"2022-05-09T18:52:50.294321Z","shell.execute_reply.started":"2022-05-09T18:52:50.290322Z","shell.execute_reply":"2022-05-09T18:52:50.293365Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"(X_train, y_train), (X_test, y_test) = mnist.load_data()\n\nprint(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)\n\nX_train = (X_train.astype(np.float32)-127.5)/127.5\nX_train = X_train.reshape(60000, 784)\n\nadam = Adam(learning_rate=0.0002, beta_1=0.5)","metadata":{"execution":{"iopub.status.busy":"2022-05-09T19:00:43.653159Z","iopub.execute_input":"2022-05-09T19:00:43.653629Z","iopub.status.idle":"2022-05-09T19:00:44.086923Z","shell.execute_reply.started":"2022-05-09T19:00:43.653588Z","shell.execute_reply":"2022-05-09T19:00:44.085784Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"generator = Sequential()\ngenerator.add(Dense(256, input_dim = randomDim, kernel_initializer=initializers.RandomNormal(stddev=0.02)))\ngenerator.add(LeakyReLU(0.2))\ngenerator.add(Dense(512))\ngenerator.add(LeakyReLU(0.2))\ngenerator.add(Dense(1024))\ngenerator.add(LeakyReLU(0.2))\ngenerator.add(Dense(784, activation='tanh'))","metadata":{"execution":{"iopub.status.busy":"2022-05-09T19:02:39.745203Z","iopub.execute_input":"2022-05-09T19:02:39.746619Z","iopub.status.idle":"2022-05-09T19:02:39.828156Z","shell.execute_reply.started":"2022-05-09T19:02:39.746568Z","shell.execute_reply":"2022-05-09T19:02:39.826926Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"discriminator = Sequential()\ndiscriminator.add(Dense(1024, input_dim=784, kernel_initializer=initializers.RandomNormal(stddev=0.02)))\ndiscriminator.add(LeakyReLU(0.2))\ndiscriminator.add(Dropout(0.3))\ndiscriminator.add(Dense(512))\ndiscriminator.add(LeakyReLU(0.2))\ndiscriminator.add(Dropout(0.3))\ndiscriminator.add(Dense(256))\ndiscriminator.add(LeakyReLU(0.2))\ndiscriminator.add(Dropout(0.3))\ndiscriminator.add(Dense(1, activation='sigmoid'))\ndiscriminator.compile(loss='binary_crossentropy', optimizer=adam)","metadata":{"execution":{"iopub.status.busy":"2022-05-09T19:08:34.869638Z","iopub.execute_input":"2022-05-09T19:08:34.870588Z","iopub.status.idle":"2022-05-09T19:08:34.977939Z","shell.execute_reply.started":"2022-05-09T19:08:34.870545Z","shell.execute_reply":"2022-05-09T19:08:34.976686Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"discriminator.trainable = False\nganInput = Input(shape=(randomDim,))\nx = generator(ganInput)\nganOutput = discriminator(x)\ngan = Model(inputs=ganInput, outputs=ganOutput)\ngan.compile(loss='binary_crossentropy', optimizer=adam)","metadata":{"execution":{"iopub.status.busy":"2022-05-09T19:08:36.961366Z","iopub.execute_input":"2022-05-09T19:08:36.962021Z","iopub.status.idle":"2022-05-09T19:08:37.021471Z","shell.execute_reply.started":"2022-05-09T19:08:36.961984Z","shell.execute_reply":"2022-05-09T19:08:37.020575Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"dLosses = []\ngLosses = []\n\n# Plot the loss from each batch\ndef plotLoss(epoch):\n    plt.figure(figsize=(10, 8))\n    plt.plot(dLosses, label='Discriminitive loss')\n    plt.plot(gLosses, label='Generative loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    #plt.savefig('images/gan_loss_epoch_%d.png' % epoch)\n\n# Create a wall of generated MNIST images\ndef plotGeneratedImages(epoch, examples=100, dim=(10, 10), figsize=(10, 10)):\n    noise = np.random.normal(0, 1, size=[examples, randomDim])\n    generatedImages = generator.predict(noise)\n    generatedImages = generatedImages.reshape(examples, 28, 28)\n\n    plt.figure(figsize=figsize)\n    for i in range(generatedImages.shape[0]):\n        plt.subplot(dim[0], dim[1], i+1)\n        plt.imshow(generatedImages[i], interpolation='nearest', cmap='gray_r')\n        plt.axis('off')\n    plt.tight_layout()\n    #plt.savefig('images/gan_generated_image_epoch_%d.png' % epoch)\n\n# Save the generator and discriminator networks (and weights) for later use\ndef saveModels(epoch):\n    generator.save('models/gan_generator_epoch_%d.h5' % epoch)\n    discriminator.save('models/gan_discriminator_epoch_%d.h5' % epoch)\n\ndef train(epochs=1, batchSize=128):\n    batchCount = X_train.shape[0] / batchSize\n    print('Epochs:', epochs, sep='')\n    print( 'Batch size:', batchSize)\n    print ('Batches per epoch:', batchCount)\n    #print()\n\n    for e in range(1, epochs+1):\n        print ('-'*15, 'Epoch %d' % e, '-'*15)\n        for mb in (range(int(batchCount))):\n            # Get a random set of input noise and images\n            noise = np.random.normal(0, 1, size=[batchSize, randomDim])\n            imageBatch = X_train[np.random.randint(0, X_train.shape[0], size=batchSize)]\n\n            # Generate fake MNIST images\n            generatedImages = generator.predict(noise)\n            # print np.shape(imageBatch), np.shape(generatedImages)\n            X = np.concatenate([imageBatch, generatedImages])\n\n            # Labels for generated and real data\n            yDis = np.zeros(2*batchSize)\n            # One-sided label smoothing\n            yDis[:batchSize] = 0.9\n\n            # Train discriminator\n            discriminator.trainable = True\n            dloss = discriminator.train_on_batch(X, yDis)\n\n            # Train generator\n            noise = np.random.normal(0, 1, size=[batchSize, randomDim])\n            yGen = np.ones(batchSize)\n            discriminator.trainable = False\n            gloss = gan.train_on_batch(noise, yGen)\n\n        # Store loss of most recent batch from this epoch\n        dLosses.append(dloss)\n        gLosses.append(gloss)\n\n        if e == 1 or e % 20 == 0:\n            plotGeneratedImages(e)\n            #saveModels(e)\n\n    # Plot losses from every epoch\n    plotLoss(e)\n\nif __name__ == '__main__':\n    train(20, 128)","metadata":{"execution":{"iopub.status.busy":"2022-05-09T19:09:31.160815Z","iopub.execute_input":"2022-05-09T19:09:31.161509Z"},"trusted":true},"execution_count":null,"outputs":[]}]}