{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# IRS Practical 3\n> 19BCE245 - Aayush Shah","metadata":{"id":"4mOYQQikJfTt"}},{"cell_type":"markdown","source":"## 1. Explore `CountVectorizer` and `TfidfVectorizer`","metadata":{"id":"kWK4ctDkJg98"}},{"cell_type":"markdown","source":"  - ### with `CountVectorizer` : ","metadata":{"id":"bVPQUI07WHTN"}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer","metadata":{"id":"bx86JA5cJRRK","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corpus = [\n  'This is the first document.',\n  'This document is the second document.',\n  'And this is the third one.',\n  'Is this the first document?',\n]","metadata":{"id":"_WpoN3i8JVwP","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vectorizer = CountVectorizer()\nX = vectorizer.fit_transform(corpus)\nvectorizer.get_feature_names_out()","metadata":{"id":"QXX04vHZK5WX","outputId":"c33722c5-f284-4981-8150-704aa27fc527","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X.toarray())","metadata":{"id":"adOKx2FMLHGj","outputId":"789d81e0-0634-461d-8e36-65c3e4d70ec3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2,2))\nX2 = vectorizer2.fit_transform(corpus)\nvectorizer2.get_feature_names_out()","metadata":{"id":"cK7c3nGWLJm4","outputId":"0d919c83-478a-4eb1-816f-350b171b824d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X2.toarray())","metadata":{"id":"yO2GO1DlLeJz","outputId":"837ef216-8460-40bb-f276-4234233694d6","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vectorizer3 = CountVectorizer(decode_error='ignore', stop_words='english', ngram_range=(1,3))\nX3 = vectorizer3.fit_transform(corpus)\nvectorizer3.get_feature_names_out()","metadata":{"id":"WcPOL8V9LqcP","outputId":"b786e1a0-61b3-4dd6-ea2b-35ae19562cb6","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X3.toarray())","metadata":{"id":"nuQGooaTMMij","outputId":"40678f2a-4fe1-43d0-c268-c25de232a9cf","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"  - ### with `TfidfVectorizer` : ","metadata":{"id":"5EddNOZtWPLz"}},{"cell_type":"code","source":"vectorizer = TfidfVectorizer()\nX = vectorizer.fit_transform(corpus)\nvectorizer.get_feature_names_out()","metadata":{"id":"xZRJuM2VWOwt","outputId":"abb92df5-2478-4d58-8ce3-4df4d648b9e0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X.shape)\nprint(vectorizer.get_feature_names_out())\nprint(X.toarray())","metadata":{"id":"nG8pTByXWuGf","outputId":"39ec04c1-9733-4eca-e110-fa0d4eed0109","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Do 1st part with file handling.","metadata":{"id":"0BTcdr2pMrw9"}},{"cell_type":"code","source":"# Making files\nfor i in range(len(corpus)):\n  f = open(\"data\"+str(i+1)+\".txt\", \"w\")\n  f.write(corpus[i])\n  f.close()","metadata":{"id":"Hi1nndOvMwFw","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reading files\nextracted_corpus = []\nfor i in range(len(corpus)):\n  f = open(\"data\"+str(i+1)+\".txt\",'r')\n  extracted_corpus.append(f.read())\n\nprint(extracted_corpus)","metadata":{"id":"PjwIF_43NHrC","outputId":"a5d12cf4-f239-4257-c455-fa56c1104eba","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Extracting document names from current directory\ndoc_names = os.listdir('.')\nprint(doc_names)\ndoc_names = [i for i in doc_names if ('.txt' in i)]\nprint(doc_names)","metadata":{"id":"Kx1F8fNbTNee","outputId":"f104a5b2-95d9-4272-9661-e90d661fa923","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- ### with `CountVectorizer` : ","metadata":{"id":"A-pDhVz7Xh14"}},{"cell_type":"code","source":"vectorizer4 = CountVectorizer(input=doc_names)\nX4 = vectorizer4.fit_transform(corpus)\nvectorizer4.get_feature_names_out()","metadata":{"id":"FF_RqiYPS27Z","outputId":"c82bbd9b-fba0-4666-d68c-7dddcc44b973","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# files = ['data1.txt','data2.txt','data3.txt','data4.txt']","metadata":{"id":"m_KHDAoxSLLl","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vectorizer4 = CountVectorizer(input=doc_names)\nX4 = vectorizer4.fit_transform(extracted_corpus)\nvectorizer4.get_feature_names_out()","metadata":{"id":"e3a_Pb2GNy7l","outputId":"182c3bc4-5451-4f18-f3e8-ca5728e18f53","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X4.toarray())","metadata":{"id":"PVWN_-PMSWFC","outputId":"687992a3-fffa-405e-ce3e-4e83e261efb8","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X4.toarray())","metadata":{"id":"L_TW4t30N54K","outputId":"e47f5002-b691-4729-ebe9-284e6dc657cc","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- ### with `TfidfVectorizer` : ","metadata":{"id":"pc2iqJVuXjJO"}},{"cell_type":"code","source":"vectorizer = TfidfVectorizer(input=doc_names)\nX5 = vectorizer.fit_transform(extracted_corpus)\nvectorizer.get_feature_names_out()","metadata":{"id":"Md4C26LPXkqX","outputId":"a3319905-991b-4425-b0f2-bee23c0dd4ed","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X5.shape)\nprint(vectorizer.get_feature_names_out())\nprint(X5.toarray())","metadata":{"id":"0dDwU6lnX9Qr","outputId":"1dafd45c-6ce7-40ed-cad3-cc2eadb82929","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Take part in competition \n> [Refer this notebook](https://www.kaggle.com/adamschroeder/countvectorizer-tfidfvectorizer-predict-comments/notebook)","metadata":{"id":"GhJxxlYNOOso"}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import  matplotlib.pyplot as plt\nimport tensorflow as tf\nimport keras\nimport re\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom wordcloud import WordCloud\nfrom nltk.stem.snowball import SnowballStemmer\nfrom sklearn.model_selection import train_test_split\nimport pickle\nimport xgboost as xgb\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_auc_score , accuracy_score , confusion_matrix , f1_score\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom skmultilearn.problem_transform import BinaryRelevance\nfrom sklearn.feature_extraction.text import TfidfVectorizer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df  =  pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/train.csv.zip')\ntrain_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df=pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/test.csv.zip')\ntest_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clean_review_text(text):\n    text = text.lower()  # covert the text to lowercase\n    text = re.sub('<.*?>','',text).strip() # remove html chars\n    text = re.sub('\\[|\\(.*\\]|\\)','', text).strip() # remove text in square brackets and parenthesis\n    text = text.translate(str.maketrans('', '', string.punctuation)) # remove punctuation marks\n    text = re.sub(\"(\\\\W)\",\" \",text).strip() # remove non-ascii chars\n    text = re.sub('\\S*\\d\\S*\\s*','', text).strip()  # remove words containing numbers\n    return text.strip()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.comment_text = train_df.comment_text.astype(str)\ntrain_df.comment_text = train_df.comment_text.apply(clean_review_text)\ntrain_df.comment_text.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nfrom nltk.stem.snowball import SnowballStemmer\nimport en_core_web_sm\nnlp = en_core_web_sm.load()\n\nsnow_stemmer = SnowballStemmer(language='english')\n\nstopwords = nlp.Defaults.stop_words\ndef apply_stemmer(text):\n    words = text.split()\n    sent = [snow_stemmer.stem(word) for word in words if not word in set(stopwords)]\n    return ' '.join(sent)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.comment_text = train_df.comment_text.apply(apply_stemmer)\ntrain_df.comment_text.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = train_df.comment_text\ny = train_df.drop(['id','comment_text'],axis = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train,x_test,y_train,y_test =  train_test_split(X,y,test_size = 0.2,random_state = 45)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_vectorizer = TfidfVectorizer(\n    strip_accents='unicode',     \n    analyzer='word',            \n    token_pattern=r'\\w{1,}',    \n    ngram_range=(1, 3),         \n    stop_words='english',\n    sublinear_tf=True)\n\nword_vectorizer.fit(x_train)    \ntrain_word_features = word_vectorizer.transform(x_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_transformed = word_vectorizer.transform(x_train)\nX_test_transformed = word_vectorizer.transform(x_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.multiclass import OneVsRestClassifier\nfrom skmultilearn.problem_transform import BinaryRelevance\nfrom sklearn.linear_model import LogisticRegression\nseed=100\n\nlog_reg = LogisticRegression(C = 10, penalty='l2', solver = 'liblinear', random_state=seed)\n\n# fit model\nclassifier_ovr_log = OneVsRestClassifier(log_reg)\nclassifier_ovr_log.fit(X_train_transformed, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train_pred_proba = classifier_ovr_log.predict_proba(X_train_transformed)\ny_test_pred_proba = classifier_ovr_log.predict_proba(X_test_transformed)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_test_predictions(df,classifier):\n    df.comment_text = df.comment_text.apply(clean_review_text)\n    df.comment_text = df.comment_text.apply(apply_stemmer)\n    X_test = df.comment_text\n    X_test_transformed = word_vectorizer.transform(X_test)\n    y_test_pred = classifier.predict_proba(X_test_transformed)\n    return y_test_pred","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred=make_test_predictions(test_df,classifier_ovr_log)\ny_pred","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_df = pd.DataFrame(y_pred,columns=y.columns)\ny_pred_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df = pd.concat([test_df.id, y_pred_df], axis=1)\nsubmission_df.to_csv('submission.csv', index = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}