{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "IRS_P10_19BCE245.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# IRS Practical 10\n",
        "> 19BCE245 - Aayush Shah\n",
        "\n",
        "- Karaoke Generator"
      ],
      "metadata": {
        "id": "7d7dJw1DU2VG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Open-Unmix PyTorch\n",
        "\n",
        "![](https://sisec18.unmix.app/static/img/hero_header.4f28952.svg)"
      ],
      "metadata": {
        "id": "CzgqL0-Q9UNo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Open-Unmix__ is a deep neural network reference implementation for music source separation, applicable for researchers, audio engineers and artists. This notebook provides easy access to pre-trained models that allow users to separate pop music into four stems: __vocals__, __drums__, __bass__ and the remaining __other__ instruments. The models were trained on the [MUSDB18](https://sigsep.github.io/datasets/musdb.html) dataset.\n",
        "\n",
        "## The Model\n",
        "\n",
        "_Open-Unmix_ is based on a three-layer bidirectional deep LSTM. The model learns to predict the magnitude spectrogram of a target, like _vocals_, from the magnitude spectrogram of a mixture input. Internally, the prediction is obtained by applying a mask on the input. The model is optimized in the magnitude domain using mean squared error and the actual separation is done in a post-processing step involving a differentiable multichannel wiener filter. To perform separation into multiple sources, multiple models are trained for each particular target. While this makes the training less comfortable, it allows great flexibility to customize the training data for each target source.\n",
        "\n",
        "## How to proceed\n",
        "\n",
        "We provide four pre-trained models:\n",
        "\n",
        "* __`umxl` (default)__  trained on private stems dataset of compressed stems. \n",
        "  [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.5069601.svg)](https://doi.org/10.5281/zenodo.5069601)\n",
        "\n",
        "* __`umxhq`__  trained on [MUSDB18-HQ](https://sigsep.github.io/datasets/musdb.html#uncompressed-wav) which comprises the same tracks as in MUSDB18 but un-compressed which yield in a full bandwidth of 22050 Hz.\n",
        "\n",
        "  [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.3267291.svg)](https://doi.org/10.5281/zenodo.3267291)\n",
        "\n",
        "* __`umx`__ is trained on the regular [MUSDB18](https://sigsep.github.io/datasets/musdb.html#compressed-stems) which is bandwidth limited to 16 kHz do to AAC compression. This model should be used for comparison with other (older) methods for evaluation in [SiSEC18](sisec18.unmix.app).\n",
        "\n",
        "  [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.3340804.svg)](https://doi.org/10.5281/zenodo.3340804)\n",
        "\n",
        "* __`umxse`__ speech enhancement model is trained on the 28-speaker version of the [Voicebank+DEMAND corpus](https://datashare.is.ed.ac.uk/handle/10283/1942?show=full).\n",
        "\n",
        "  [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.3786908.svg)](https://doi.org/10.5281/zenodo.3786908)\n",
        "\n",
        "All models are downloaded automatically.\n",
        "\n",
        "### Colab Limitations \n",
        "\n",
        "* The disk and RAM is limited in colab. Loading the four separation models `vocals`, `drums`, `bass` and `other` is already using 400 MB of disk and RAM. \n",
        "* A major step in the separation is the post-processing, contolled by the parameters `niter`. For faster inference (at the expense of separation quality) it is adviced to use `niter=0`.\n",
        "* Another way to prevent colab from crashing is to only perform separation on smaller excerpts. In the following examples we privide a way to set the start and stop duration of the audio being separated. We suggest __not to separate segements of longer than 30s__.\n",
        "\n"
      ],
      "metadata": {
        "id": "62GqQkhyT7I0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installation and Imports (RUN THESE CELLS FIRST)"
      ],
      "metadata": {
        "id": "N-026sn4gBPN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install musdb\n",
        "!pip install youtube-dl\n",
        "!pip install openunmix\n",
        "!pip install nlpaug\n",
        "!pip install pydub\n",
        "!pip install google-colab"
      ],
      "metadata": {
        "id": "CvF3oVR62ei7",
        "execution": {
          "iopub.status.busy": "2022-05-06T21:09:43.238315Z",
          "iopub.execute_input": "2022-05-06T21:09:43.238611Z",
          "iopub.status.idle": "2022-05-06T21:11:28.612635Z",
          "shell.execute_reply.started": "2022-05-06T21:09:43.238581Z",
          "shell.execute_reply": "2022-05-06T21:11:28.611359Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "import numpy as np\n",
        "import scipy\n",
        "import youtube_dl\n",
        "import stempeg\n",
        "import os\n",
        "import librosa\n",
        "from google.colab import files\n",
        "from IPython.display import Audio, display\n",
        "import matplotlib.pyplot as plt\n",
        "import nlpaug.augmenter.audio as naa\n",
        "from pydub import AudioSegment \n",
        "import subprocess\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
      ],
      "metadata": {
        "id": "-vSlkpwm3B6z",
        "execution": {
          "iopub.status.busy": "2022-05-06T21:11:28.614956Z",
          "iopub.execute_input": "2022-05-06T21:11:28.615229Z",
          "iopub.status.idle": "2022-05-06T21:11:28.6776Z",
          "shell.execute_reply.started": "2022-05-06T21:11:28.615196Z",
          "shell.execute_reply": "2022-05-06T21:11:28.676743Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Separate MUSDB18 tracks"
      ],
      "metadata": {
        "id": "1b_uX0MnElFe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get a musdb18 7 second preview track"
      ],
      "metadata": {
        "id": "dR5kVtrPVSI0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import musdb\n",
        "mus = musdb.DB(download=True, subsets='test')\n",
        "\n",
        "for i in mus:\n",
        "    print(i)\n",
        "\n",
        "track = mus[25]\n",
        "print(track.name)\n",
        "display(Audio(track.audio.T, rate=track.rate))"
      ],
      "metadata": {
        "id": "cPrPR81IVRM8",
        "execution": {
          "iopub.status.busy": "2022-05-06T21:11:28.679081Z",
          "iopub.execute_input": "2022-05-06T21:11:28.67932Z",
          "iopub.status.idle": "2022-05-06T21:11:47.24282Z",
          "shell.execute_reply.started": "2022-05-06T21:11:28.679288Z",
          "shell.execute_reply": "2022-05-06T21:11:47.242022Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Apply separation into four stems\n",
        "\n",
        "open-unmix is auto-downloading a model for each available target:\n",
        "\n",
        "* vocals\n",
        "* drums\n",
        "* bass\n",
        "* other"
      ],
      "metadata": {
        "id": "4rh8W7jZVbfF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openunmix import predict\n",
        "estimates = predict.separate(\n",
        "    torch.as_tensor(track.audio).float(),\n",
        "    rate=track.rate,\n",
        "    device=device\n",
        ")   \n",
        "for target, estimate in estimates.items():\n",
        "    print(target)\n",
        "    audio = estimate.detach().cpu().numpy()[0]\n",
        "    display(Audio(audio, rate=track.rate))"
      ],
      "metadata": {
        "id": "vQJkMHKP3qUH",
        "execution": {
          "iopub.status.busy": "2022-05-06T21:11:47.245151Z",
          "iopub.execute_input": "2022-05-06T21:11:47.245567Z",
          "iopub.status.idle": "2022-05-06T21:13:02.674345Z",
          "shell.execute_reply.started": "2022-05-06T21:11:47.245526Z",
          "shell.execute_reply": "2022-05-06T21:13:02.673175Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Apply separation into vocals/accompaniment\n",
        "\n",
        "Even open-unmix does not provide a separate model for the accompaniment, we can use the spectral `residual` model in the post-processing to force a linear sum of all separated sources - e.g. this can be used for vocal/accompaniment separation. Note, that the sepearation performance is decreased when using the residual model."
      ],
      "metadata": {
        "id": "L4gkA_DrWJXM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "estimates = predict.separate(\n",
        "    torch.as_tensor(track.audio).float(),\n",
        "    rate=track.rate,\n",
        "    targets=['vocals'], \n",
        "    residual=True,\n",
        "    device=device,\n",
        ")\n",
        "for target, estimate in estimates.items():\n",
        "    print(target)\n",
        "    display(Audio(estimate.detach().cpu().numpy()[0], rate=track.rate))"
      ],
      "metadata": {
        "id": "9PPs4Mb8Whu0",
        "execution": {
          "iopub.status.busy": "2022-05-06T21:13:02.675755Z",
          "iopub.execute_input": "2022-05-06T21:13:02.676299Z",
          "iopub.status.idle": "2022-05-06T21:13:07.290817Z",
          "shell.execute_reply.started": "2022-05-06T21:13:02.676262Z",
          "shell.execute_reply": "2022-05-06T21:13:07.290051Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another way to achive vocal/accompanimnet separation is to sepearate into four stems and sum up the non-vocal stems."
      ],
      "metadata": {
        "id": "JSo2ElRVYcta"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "estimates = predict.separate(\n",
        "    audio=torch.as_tensor(track.audio).float(), \n",
        "    rate=track.rate,\n",
        "    targets=['vocals', 'drums', 'bass', 'other'], \n",
        "    residual=True,\n",
        "    device=device\n",
        ")\n",
        "print('vocals')\n",
        "display(Audio(estimates['vocals'].detach().cpu().numpy()[0], rate=track.rate))\n",
        "acc = np.sum(\n",
        "    [audio.detach().cpu().numpy()[0] for target, audio in estimates.items() if not target=='vocals'],\n",
        "    axis=0\n",
        ")\n",
        "print('accompaniment')\n",
        "display(Audio(acc, rate=track.rate))"
      ],
      "metadata": {
        "id": "U9XNhoVYXTxn",
        "execution": {
          "iopub.status.busy": "2022-05-06T21:13:07.292228Z",
          "iopub.execute_input": "2022-05-06T21:13:07.293018Z",
          "iopub.status.idle": "2022-05-06T21:13:25.279105Z",
          "shell.execute_reply.started": "2022-05-06T21:13:07.292976Z",
          "shell.execute_reply": "2022-05-06T21:13:25.278164Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Separate Youtube Video"
      ],
      "metadata": {
        "id": "dcnlkuT-Euth"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML\n",
        "url = \"W8a4sUabCUo\" #@param {type:\"string\"}\n",
        "start = 60 #@param {type:\"number\"}\n",
        "stop = 90 #@param {type:\"number\"}\n",
        "embed_url = \"https://www.youtube.com/embed/%s?start=%d&end=%d\" %(url,start,stop)\n",
        "# embed_url = \"https://www.youtube.com/embed/%s?rel=0&start=%d&end=%d\" % (url, start, stop)\n",
        "HTML('<iframe width=\"550\" height=\"315\" src=' + embed_url + 'frameborder=\"0\" allowfullscreen></iframe>')"
      ],
      "metadata": {
        "id": "TfMvyXqKcK1Z",
        "execution": {
          "iopub.status.busy": "2022-05-06T21:13:25.280895Z",
          "iopub.execute_input": "2022-05-06T21:13:25.281161Z",
          "iopub.status.idle": "2022-05-06T21:13:25.289426Z",
          "shell.execute_reply.started": "2022-05-06T21:13:25.281124Z",
          "shell.execute_reply": "2022-05-06T21:13:25.288748Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import stempeg\n",
        "\n",
        "def my_hook(d):\n",
        "    if d['status'] == 'finished':\n",
        "        print('Done downloading...')\n",
        "\n",
        "\n",
        "ydl_opts = {\n",
        "    'format': 'bestaudio/best',\n",
        "    'postprocessors': [{\n",
        "        'key': 'FFmpegExtractAudio',\n",
        "        'preferredcodec': 'wav',\n",
        "        'preferredquality': '44100',\n",
        "    }],\n",
        "    'outtmpl': '%(title)s.wav',\n",
        "    'progress_hooks': [my_hook],\n",
        "}\n",
        "with youtube_dl.YoutubeDL(ydl_opts) as ydl:\n",
        "    info = ydl.extract_info(url, download=False)\n",
        "    status = ydl.download([url])\n",
        "\n",
        "audioYT, samplerate = stempeg.read_stems(\n",
        "    info.get('title', None) + '.wav', \n",
        "    start=start,\n",
        "    duration=(stop-start),\n",
        "    sample_rate=44100.0,\n",
        "    dtype=np.float32\n",
        ")\n",
        "display(Audio(audioYT.T, rate=samplerate))\n",
        "estimates = predict.separate(\n",
        "    torch.as_tensor(audioYT).float(),\n",
        "    rate=samplerate,\n",
        "    device=device\n",
        ")   \n",
        "for target, estimate in estimates.items():\n",
        "    print(target)\n",
        "    display(Audio(estimate.detach().cpu().numpy()[0], rate=samplerate))"
      ],
      "metadata": {
        "cellView": "both",
        "id": "Rm03VfJu8M9e",
        "execution": {
          "iopub.status.busy": "2022-05-06T21:13:25.290649Z",
          "iopub.execute_input": "2022-05-06T21:13:25.291074Z",
          "iopub.status.idle": "2022-05-06T21:15:59.642716Z",
          "shell.execute_reply.started": "2022-05-06T21:13:25.291032Z",
          "shell.execute_reply": "2022-05-06T21:15:59.641853Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Apply separation into 2 classes \n",
        "##1) Vocals\n",
        "##2) Accompaniment"
      ],
      "metadata": {
        "id": "KGNEleoWaaVR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "audioYT, samplerate = stempeg.read_stems(\n",
        "    info.get('title', None) + '.wav', \n",
        "    start=start,\n",
        "    duration=(stop-start),\n",
        "    sample_rate=44100.0,\n",
        "    dtype=np.float32\n",
        ")\n",
        "display(Audio(audioYT.T, rate=samplerate))\n",
        "estimates = predict.separate(\n",
        "    torch.as_tensor(audioYT).float(),\n",
        "    rate=samplerate,\n",
        "    device=device\n",
        ")   \n",
        "print('vocals')\n",
        "display(Audio(estimates['vocals'].detach().cpu().numpy()[0], rate=samplerate))\n",
        "acc = np.sum(\n",
        "    [audio.detach().cpu().numpy()[0] for target, audio in estimates.items() if not target=='vocals'],\n",
        "    axis=0\n",
        ")\n",
        "print('accompaniment')\n",
        "display(Audio(acc, rate=track.rate))"
      ],
      "metadata": {
        "id": "cjGjOlY1XRti",
        "execution": {
          "iopub.status.busy": "2022-05-06T21:15:59.644094Z",
          "iopub.execute_input": "2022-05-06T21:15:59.644798Z",
          "iopub.status.idle": "2022-05-06T21:17:08.551704Z",
          "shell.execute_reply.started": "2022-05-06T21:15:59.644759Z",
          "shell.execute_reply": "2022-05-06T21:17:08.550909Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download separations"
      ],
      "metadata": {
        "id": "CEiEjSWQb1BP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# target_path = str(\"target.mp3\")\n",
        "\n",
        "# estimates_numpy = {}\n",
        "# for target, estimate in estimates.items():\n",
        "#     estimates_numpy[target] = torch.squeeze(estimate).detach().cpu().numpy().T\n",
        "\n",
        "# stempeg.write_stems(\n",
        "#     target_path,\n",
        "#     estimates_numpy,\n",
        "#     writer=stempeg.FilesWriter(multiprocess=True, output_sample_rate=44100),\n",
        "# )\n",
        "\n",
        "# for target, estimate in estimates.items():\n",
        "#     files.download(target + '.mp3')"
      ],
      "metadata": {
        "id": "dvUDAibHbzA5",
        "execution": {
          "iopub.status.busy": "2022-05-06T21:17:08.554277Z",
          "iopub.execute_input": "2022-05-06T21:17:08.554653Z",
          "iopub.status.idle": "2022-05-06T21:17:08.558134Z",
          "shell.execute_reply.started": "2022-05-06T21:17:08.55462Z",
          "shell.execute_reply": "2022-05-06T21:17:08.55729Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Separate from uploaded file"
      ],
      "metadata": {
        "id": "6wHnnug6FEPX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "'''\n",
        "uploaded = files.upload()\n",
        "'''"
      ],
      "metadata": {
        "id": "TMmjLKx0FHMG",
        "execution": {
          "iopub.status.busy": "2022-05-06T21:19:42.188265Z",
          "iopub.execute_input": "2022-05-06T21:19:42.188567Z",
          "iopub.status.idle": "2022-05-06T21:19:42.195995Z",
          "shell.execute_reply.started": "2022-05-06T21:19:42.188536Z",
          "shell.execute_reply": "2022-05-06T21:19:42.194898Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "start = 0 #@param {type:\"number\"}\n",
        "stop =  10#@param {type:\"number\"}\n",
        "audio, rate = stempeg.read_stems(\n",
        "    list(uploaded.keys())[0],\n",
        "    sample_rate=44100,\n",
        "    start=start,\n",
        "    duration=stop-start,\n",
        ")\n",
        "display(Audio(audio.T, rate=rate))\n",
        "estimates = predict.separate(\n",
        "    audio=torch.as_tensor(audio).float(),\n",
        "    rate=44100,\n",
        "    device=device,\n",
        ")\n",
        "for target, estimate in estimates.items():\n",
        "    print(target)\n",
        "    display(Audio(estimate.detach().cpu().numpy()[0], rate=rate))\n",
        "'''"
      ],
      "metadata": {
        "id": "4ShoVlZuGRjF",
        "execution": {
          "iopub.status.busy": "2022-05-06T21:19:43.893126Z",
          "iopub.execute_input": "2022-05-06T21:19:43.893808Z",
          "iopub.status.idle": "2022-05-06T21:19:43.901124Z",
          "shell.execute_reply.started": "2022-05-06T21:19:43.893759Z",
          "shell.execute_reply": "2022-05-06T21:19:43.900139Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Apply separation into 2 classes \n",
        "##1) Vocals\n",
        "##2) Accompaniment"
      ],
      "metadata": {
        "id": "mdXlep9wavS_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "audio, rate = stempeg.read_stems(\n",
        "    list(uploaded.keys())[0],\n",
        "    sample_rate=44100,\n",
        "    start=start,\n",
        "    duration=stop-start,\n",
        ")\n",
        "display(Audio(audio.T, rate=rate))\n",
        "estimates = predict.separate(\n",
        "    audio=torch.as_tensor(audio).float(),\n",
        "    rate=44100,\n",
        "    device=device,\n",
        ")\n",
        "print('vocals')\n",
        "display(Audio(estimates['vocals'].detach().cpu().numpy()[0], rate=samplerate))\n",
        "acc = np.sum(\n",
        "    [audio.detach().cpu().numpy()[0] for target, audio in estimates.items() if not target=='vocals'],\n",
        "    axis=0\n",
        ")\n",
        "print('accompaniment')\n",
        "display(Audio(acc, rate=track.rate))\n",
        "'''"
      ],
      "metadata": {
        "id": "ld9N2aTyZYUN",
        "execution": {
          "iopub.status.busy": "2022-05-06T21:19:46.740115Z",
          "iopub.execute_input": "2022-05-06T21:19:46.740813Z",
          "iopub.status.idle": "2022-05-06T21:19:46.747749Z",
          "shell.execute_reply.started": "2022-05-06T21:19:46.740769Z",
          "shell.execute_reply": "2022-05-06T21:19:46.746899Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}