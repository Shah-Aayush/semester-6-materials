{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a2a3ea36",
      "metadata": {
        "papermill": {
          "duration": 0.068177,
          "end_time": "2022-03-12T06:11:12.817545",
          "exception": false,
          "start_time": "2022-03-12T06:11:12.749368",
          "status": "completed"
        },
        "tags": [],
        "id": "a2a3ea36"
      },
      "source": [
        "# IRS Practical 2\n",
        "> 19BCE245 - Aayush Shah\n",
        "- Text Preprocessing using NLTK. Visualization\n",
        "  - Word Cloud\n",
        "  - Histogram of top N frequent terms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "267ce879",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-12T06:11:12.977895Z",
          "iopub.status.busy": "2022-03-12T06:11:12.975837Z",
          "iopub.status.idle": "2022-03-12T06:11:25.900630Z",
          "shell.execute_reply": "2022-03-12T06:11:25.901345Z",
          "shell.execute_reply.started": "2022-03-12T05:42:52.862565Z"
        },
        "papermill": {
          "duration": 13.015011,
          "end_time": "2022-03-12T06:11:25.901673",
          "exception": false,
          "start_time": "2022-03-12T06:11:12.886662",
          "status": "completed"
        },
        "tags": [],
        "id": "267ce879"
      },
      "outputs": [],
      "source": [
        "#import required stuff\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import feature_extraction, linear_model, model_selection, preprocessing\n",
        "\n",
        "import re #for working with regular expression\n",
        "import nltk #for natural language processing (nlp)\n",
        "import spacy #also for nlp\n",
        "import string #This is a module, Python also has built-in class str, these are different"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d69ec050",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-12T06:11:25.985264Z",
          "iopub.status.busy": "2022-03-12T06:11:25.984372Z",
          "iopub.status.idle": "2022-03-12T06:11:26.043286Z",
          "shell.execute_reply": "2022-03-12T06:11:26.042639Z",
          "shell.execute_reply.started": "2022-03-12T05:43:04.242719Z"
        },
        "papermill": {
          "duration": 0.103108,
          "end_time": "2022-03-12T06:11:26.043472",
          "exception": false,
          "start_time": "2022-03-12T06:11:25.940364",
          "status": "completed"
        },
        "tags": [],
        "id": "d69ec050"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\n",
        "test_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0e9fb7c",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-12T06:11:26.129787Z",
          "iopub.status.busy": "2022-03-12T06:11:26.128966Z",
          "iopub.status.idle": "2022-03-12T06:11:26.177908Z",
          "shell.execute_reply": "2022-03-12T06:11:26.177260Z",
          "shell.execute_reply.started": "2022-03-12T05:43:04.306697Z"
        },
        "papermill": {
          "duration": 0.095458,
          "end_time": "2022-03-12T06:11:26.178086",
          "exception": false,
          "start_time": "2022-03-12T06:11:26.082628",
          "status": "completed"
        },
        "tags": [],
        "id": "e0e9fb7c",
        "outputId": "ddcef67a-a751-43bc-d3e9-1b83f5aa6ce4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train dataframe : \n",
            "    id keyword location                                               text  \\\n",
            "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
            "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
            "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
            "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
            "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
            "\n",
            "   target  \n",
            "0       1  \n",
            "1       1  \n",
            "2       1  \n",
            "3       1  \n",
            "4       1  \n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 7613 entries, 0 to 7612\n",
            "Data columns (total 5 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   id        7613 non-null   int64 \n",
            " 1   keyword   7552 non-null   object\n",
            " 2   location  5080 non-null   object\n",
            " 3   text      7613 non-null   object\n",
            " 4   target    7613 non-null   int64 \n",
            "dtypes: int64(2), object(3)\n",
            "memory usage: 297.5+ KB\n",
            "None\n",
            "test dataframe :     id keyword location                                               text\n",
            "0   0     NaN      NaN                 Just happened a terrible car crash\n",
            "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
            "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
            "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
            "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3263 entries, 0 to 3262\n",
            "Data columns (total 4 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   id        3263 non-null   int64 \n",
            " 1   keyword   3237 non-null   object\n",
            " 2   location  2158 non-null   object\n",
            " 3   text      3263 non-null   object\n",
            "dtypes: int64(1), object(3)\n",
            "memory usage: 102.1+ KB\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "print('train dataframe : \\n',train_df.head(5))\n",
        "print(train_df.info())\n",
        "print('test dataframe : ',test_df.head(5))\n",
        "print(test_df.info())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f89ff7ac",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-12T06:11:26.266740Z",
          "iopub.status.busy": "2022-03-12T06:11:26.265656Z",
          "iopub.status.idle": "2022-03-12T06:11:26.270417Z",
          "shell.execute_reply": "2022-03-12T06:11:26.269502Z",
          "shell.execute_reply.started": "2022-03-12T05:43:04.3555Z"
        },
        "papermill": {
          "duration": 0.050924,
          "end_time": "2022-03-12T06:11:26.270652",
          "exception": false,
          "start_time": "2022-03-12T06:11:26.219728",
          "status": "completed"
        },
        "tags": [],
        "id": "f89ff7ac",
        "outputId": "baa861ad-a0e8-4099-90de-0df675b89272"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "7613\n",
            "3263\n"
          ]
        }
      ],
      "source": [
        "print(len(train_df.index))\n",
        "print(len(test_df.index))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3e32a5e",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-12T06:11:26.363727Z",
          "iopub.status.busy": "2022-03-12T06:11:26.362418Z",
          "iopub.status.idle": "2022-03-12T06:11:26.366577Z",
          "shell.execute_reply": "2022-03-12T06:11:26.365844Z",
          "shell.execute_reply.started": "2022-03-12T05:43:04.362782Z"
        },
        "papermill": {
          "duration": 0.054422,
          "end_time": "2022-03-12T06:11:26.366724",
          "exception": false,
          "start_time": "2022-03-12T06:11:26.312302",
          "status": "completed"
        },
        "tags": [],
        "id": "b3e32a5e"
      },
      "outputs": [],
      "source": [
        "# Merging train and test dataframe for performing text-preprocessing\n",
        "train_df_copy = train_df\n",
        "train_df = train_df.drop('target', axis = 1)\n",
        "frames = [train_df,test_df]\n",
        "train_df = pd.concat(frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76a9ebe5",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-12T06:11:26.547846Z",
          "iopub.status.busy": "2022-03-12T06:11:26.546548Z",
          "iopub.status.idle": "2022-03-12T06:11:26.550942Z",
          "shell.execute_reply": "2022-03-12T06:11:26.551811Z",
          "shell.execute_reply.started": "2022-03-12T05:43:04.375338Z"
        },
        "papermill": {
          "duration": 0.061205,
          "end_time": "2022-03-12T06:11:26.552139",
          "exception": false,
          "start_time": "2022-03-12T06:11:26.490934",
          "status": "completed"
        },
        "tags": [],
        "id": "76a9ebe5",
        "outputId": "2c6a4475-23b9-4adb-c28c-3cfeb74b8ddb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0    our deeds are the reason of this #earthquake m...\n",
            "1               forest fire near la ronge sask. canada\n",
            "2    all residents asked to 'shelter in place' are ...\n",
            "Name: lowered_text, dtype: object\n"
          ]
        }
      ],
      "source": [
        "# Converting everything in Lower case\n",
        "train_df['lowered_text'] = train_df['text'].str.lower()\n",
        "print(train_df['lowered_text'].head(3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96cff4a1",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-12T06:11:26.703313Z",
          "iopub.status.busy": "2022-03-12T06:11:26.671571Z",
          "iopub.status.idle": "2022-03-12T06:11:26.707837Z",
          "shell.execute_reply": "2022-03-12T06:11:26.708454Z",
          "shell.execute_reply.started": "2022-03-12T05:43:04.393587Z"
        },
        "papermill": {
          "duration": 0.112202,
          "end_time": "2022-03-12T06:11:26.708653",
          "exception": false,
          "start_time": "2022-03-12T06:11:26.596451",
          "status": "completed"
        },
        "tags": [],
        "id": "96cff4a1",
        "outputId": "5d4acc4a-0e3c-4c87-d71f-d3c745cb876a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0    our deeds are the reason of this #earthquake m...\n",
            "1               forest fire near la ronge sask. canada\n",
            "2    all residents asked to 'shelter in place' are ...\n",
            "3    13,000 people receive #wildfires evacuation or...\n",
            "4    just got sent this photo from ruby #alaska as ...\n",
            "5    #rockyfire update => california hwy. 20 closed...\n",
            "6    #flood #disaster heavy rain causes flash flood...\n",
            "7    i'm on top of the hill and i can see a fire in...\n",
            "8    there's an emergency evacuation happening now ...\n",
            "9    i'm afraid that the tornado is coming to our a...\n",
            "Name: lowered_text, dtype: object\n",
            "0    our deeds are the reason of this earthquake ma...\n",
            "1                forest fire near la ronge sask canada\n",
            "2    all residents asked to shelter in place are be...\n",
            "3    13000 people receive wildfires evacuation orde...\n",
            "4    just got sent this photo from ruby alaska as s...\n",
            "5    rockyfire update  california hwy 20 closed in ...\n",
            "6    flood disaster heavy rain causes flash floodin...\n",
            "7    im on top of the hill and i can see a fire in ...\n",
            "8    theres an emergency evacuation happening now i...\n",
            "9     im afraid that the tornado is coming to our area\n",
            "Name: lowered_text, dtype: object\n"
          ]
        }
      ],
      "source": [
        "# Removing punctuation\n",
        "punctuation=string.punctuation\n",
        "mapping=str.maketrans(\"\",\"\",punctuation)\n",
        "\n",
        "def remove_punctuation(in_str):\n",
        "    return in_str.translate(mapping)\n",
        "\n",
        "print(train_df['lowered_text'].head(10))   \n",
        "train_df['lowered_text']=train_df[\"lowered_text\"].apply(lambda x: remove_punctuation(x))\n",
        "print(train_df['lowered_text'].head(10)) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b645274",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-12T06:11:26.799402Z",
          "iopub.status.busy": "2022-03-12T06:11:26.798522Z",
          "iopub.status.idle": "2022-03-12T06:11:27.113841Z",
          "shell.execute_reply": "2022-03-12T06:11:27.114504Z",
          "shell.execute_reply.started": "2022-03-12T05:43:04.444822Z"
        },
        "papermill": {
          "duration": 0.365165,
          "end_time": "2022-03-12T06:11:27.114689",
          "exception": false,
          "start_time": "2022-03-12T06:11:26.749524",
          "status": "completed"
        },
        "tags": [],
        "id": "7b645274",
        "outputId": "a9381c8e-287d-40db-ab65-f2e925b97573"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0    our deeds are the reason of this earthquake ma...\n",
            "1                forest fire near la ronge sask canada\n",
            "2    all residents asked to shelter in place are be...\n",
            "3    13000 people receive wildfires evacuation orde...\n",
            "4    just got sent this photo from ruby alaska as s...\n",
            "5    rockyfire update  california hwy 20 closed in ...\n",
            "6    flood disaster heavy rain causes flash floodin...\n",
            "7    im on top of the hill and i can see a fire in ...\n",
            "8    theres an emergency evacuation happening now i...\n",
            "9     im afraid that the tornado is coming to our area\n",
            "Name: lowered_text, dtype: object\n",
            "0        deeds reason earthquake may allah forgive us \n",
            "1               forest fire near la ronge sask canada \n",
            "2    residents asked shelter place notified officer...\n",
            "3    13000 people receive wildfires evacuation orde...\n",
            "4    got sent photo ruby alaska smoke wildfires pou...\n",
            "5    rockyfire update california hwy 20 closed dire...\n",
            "6    flood disaster heavy rain causes flash floodin...\n",
            "7                          im top hill see fire woods \n",
            "8    theres emergency evacuation happening building...\n",
            "9                       im afraid tornado coming area \n",
            "Name: lowered_text_stop_removed, dtype: object\n"
          ]
        }
      ],
      "source": [
        "# Removing Stop words\n",
        "from nltk.corpus import stopwords\n",
        "stopwords_eng=stopwords.words('english')\n",
        "\n",
        "print(train_df[\"lowered_text\"].head(10)) #before\n",
        "\n",
        "def remove_stopwords(in_str):\n",
        "    new_str=''\n",
        "    words=in_str.split()\n",
        "    for tx in words:\n",
        "        if tx not in stopwords_eng:\n",
        "            new_str=new_str + tx + \" \"\n",
        "    return new_str\n",
        "\n",
        "train_df['lowered_text_stop_removed']=train_df[\"lowered_text\"].apply(lambda x: remove_stopwords(x))\n",
        "print(train_df[\"lowered_text_stop_removed\"].head(10)) #after"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ef020de",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-12T06:11:27.204873Z",
          "iopub.status.busy": "2022-03-12T06:11:27.203795Z",
          "iopub.status.idle": "2022-03-12T06:11:27.334983Z",
          "shell.execute_reply": "2022-03-12T06:11:27.335921Z",
          "shell.execute_reply.started": "2022-03-12T05:43:04.738941Z"
        },
        "papermill": {
          "duration": 0.179619,
          "end_time": "2022-03-12T06:11:27.336240",
          "exception": false,
          "start_time": "2022-03-12T06:11:27.156621",
          "status": "completed"
        },
        "tags": [],
        "id": "5ef020de",
        "outputId": "fbdb1ee0-69fd-45e3-8ebe-64dd5071e3b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'list'> [('like', 490), ('amp', 434), ('im', 419), ('fire', 357), ('get', 335), ('new', 326), ('via', 324), ('news', 282), ('people', 278), ('one', 277)]\n",
            "Most common words :  ['like', 'amp', 'im', 'fire', 'get', 'new', 'via', 'news', 'people', 'one']\n"
          ]
        }
      ],
      "source": [
        "# Removing most frequent 10 words\n",
        "from collections import Counter\n",
        "counter=Counter()\n",
        "for text in train_df[\"lowered_text_stop_removed\"]:\n",
        "    for word in text.split():\n",
        "        counter[word]+=1\n",
        "most_cmn_list=counter.most_common(10)\n",
        "print(type(most_cmn_list), most_cmn_list)\n",
        "most_cmn_words_list=[]\n",
        "for word, freq in most_cmn_list:\n",
        "    most_cmn_words_list.append(word)\n",
        "print('Most common words : ', most_cmn_words_list)\n",
        "\n",
        "def remove_frequent(in_str):\n",
        "    new_str=''\n",
        "    for word in in_str.split():\n",
        "        if word not in most_cmn_words_list:\n",
        "            new_str=new_str + word + \" \"\n",
        "    return new_str\n",
        "\n",
        "train_df[\"lowered_text_stop_removed_freq_removed\"]=train_df['lowered_text_stop_removed'].apply(lambda x: remove_frequent(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ab5ce81",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-12T06:11:27.472973Z",
          "iopub.status.busy": "2022-03-12T06:11:27.447454Z",
          "iopub.status.idle": "2022-03-12T06:11:27.504964Z",
          "shell.execute_reply": "2022-03-12T06:11:27.504158Z",
          "shell.execute_reply.started": "2022-03-12T05:43:04.853125Z"
        },
        "papermill": {
          "duration": 0.123343,
          "end_time": "2022-03-12T06:11:27.505190",
          "exception": false,
          "start_time": "2022-03-12T06:11:27.381847",
          "status": "completed"
        },
        "tags": [],
        "id": "3ab5ce81",
        "outputId": "aeba4d50-565b-4a78-f3ed-c3cd5206507b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Most rare words :  ['httptcotjpylu9fox', 'httptcopfavw5qyqe', 'httptcohkut5msdtp', 'issuicide', 'rajman', 'hasaka', 'risen', 'fasteners', 'xrwn', 'httptcoutbxlcbiuy']\n"
          ]
        }
      ],
      "source": [
        "# Removing 10 most rare words\n",
        "most_rare_list=counter.most_common()[-10:]\n",
        "most_rare_words=[]\n",
        "for word, freq in most_rare_list:\n",
        "    most_rare_words.append(word)\n",
        "print('Most rare words : ',most_rare_words)\n",
        "\n",
        "def remove_rare(in_text):\n",
        "    new_text=\"\"\n",
        "    for word in in_text.split():\n",
        "        if word not in most_rare_words:\n",
        "            new_text=new_text + word + \" \"\n",
        "    return new_text\n",
        "\n",
        "train_df[\"lowered_stop_freq_rare_removed\"]=train_df[\"lowered_text_stop_removed_freq_removed\"].apply(lambda x: remove_rare(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e825069",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-12T06:11:27.597750Z",
          "iopub.status.busy": "2022-03-12T06:11:27.596638Z",
          "iopub.status.idle": "2022-03-12T06:11:27.599109Z",
          "shell.execute_reply": "2022-03-12T06:11:27.599689Z",
          "shell.execute_reply.started": "2022-03-12T05:43:04.916268Z"
        },
        "papermill": {
          "duration": 0.050643,
          "end_time": "2022-03-12T06:11:27.599858",
          "exception": false,
          "start_time": "2022-03-12T06:11:27.549215",
          "status": "completed"
        },
        "tags": [],
        "id": "5e825069"
      },
      "outputs": [],
      "source": [
        "# Stemming using PorterStemmer [IGNORE... SKIP TO SnowballStemmer.]\n",
        "\n",
        "# from nltk.stem.porter import PorterStemmer\n",
        "# stemmer=PorterStemmer()\n",
        "\n",
        "# def do_stemming(in_str):\n",
        "#     new_str=\"\"\n",
        "#     for word in in_str.split():\n",
        "#         new_str=new_str + stemmer.stem(word) + \" \"\n",
        "#     return new_str\n",
        "\n",
        "# train_df[\"Stemmed\"]=train_df[\"lowered_stop_freq_rare_removed\"].apply(lambda x: do_stemming(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "800cf0b8",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-12T06:11:27.689346Z",
          "iopub.status.busy": "2022-03-12T06:11:27.688289Z",
          "iopub.status.idle": "2022-03-12T06:11:27.693449Z",
          "shell.execute_reply": "2022-03-12T06:11:27.692824Z",
          "shell.execute_reply.started": "2022-03-12T05:43:04.922228Z"
        },
        "papermill": {
          "duration": 0.051128,
          "end_time": "2022-03-12T06:11:27.693603",
          "exception": false,
          "start_time": "2022-03-12T06:11:27.642475",
          "status": "completed"
        },
        "tags": [],
        "id": "800cf0b8"
      },
      "outputs": [],
      "source": [
        "# Stemming using SnowballStemmer [IGNORE... SKIP TO Lemmatization.]\n",
        "\n",
        "# from nltk.stem.snowball import SnowballStemmer\n",
        "# stemmer_sb=SnowballStemmer(language='english')\n",
        "\n",
        "# def do_stemming_sb(in_str):\n",
        "#     new_str=\"\"\n",
        "#     for word in in_str.split():\n",
        "#         new_str=new_str + stemmer_sb.stem(word) + \" \"\n",
        "#     return new_str\n",
        "\n",
        "# train_df[\"Stemmed_sb\"]=train_df[\"lowered_stop_freq_rare_removed\"].apply(lambda x: do_stemming_sb(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d1598c3",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-12T06:11:27.782608Z",
          "iopub.status.busy": "2022-03-12T06:11:27.781548Z",
          "iopub.status.idle": "2022-03-12T06:11:31.340011Z",
          "shell.execute_reply": "2022-03-12T06:11:31.339433Z",
          "shell.execute_reply.started": "2022-03-12T05:43:04.931643Z"
        },
        "papermill": {
          "duration": 3.604386,
          "end_time": "2022-03-12T06:11:31.340206",
          "exception": false,
          "start_time": "2022-03-12T06:11:27.735820",
          "status": "completed"
        },
        "tags": [],
        "id": "3d1598c3"
      },
      "outputs": [],
      "source": [
        "# Lemmatization [IGNORE... SKIP TO Lemmatization with POS]\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lem=WordNetLemmatizer()\n",
        "\n",
        "def do_lemmatizing(in_str):\n",
        "    new_str=\"\"\n",
        "    for word in in_str.split():\n",
        "        new_str=new_str + lem.lemmatize(word) + \" \"\n",
        "    return new_str\n",
        "\n",
        "train_df[\"Lemmatized\"]=train_df[\"lowered_stop_freq_rare_removed\"].apply(lambda x: do_lemmatizing(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44705918",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-12T06:11:31.434631Z",
          "iopub.status.busy": "2022-03-12T06:11:31.433431Z",
          "iopub.status.idle": "2022-03-12T06:11:31.435850Z",
          "shell.execute_reply": "2022-03-12T06:11:31.436502Z",
          "shell.execute_reply.started": "2022-03-12T05:43:07.312256Z"
        },
        "papermill": {
          "duration": 0.051888,
          "end_time": "2022-03-12T06:11:31.436686",
          "exception": false,
          "start_time": "2022-03-12T06:11:31.384798",
          "status": "completed"
        },
        "tags": [],
        "id": "44705918"
      },
      "outputs": [],
      "source": [
        "# from nltk.corpus import wordnet\n",
        "\n",
        "# wordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV}\n",
        "# pos_tagged_text = nltk.pos_tag(train_df.loc[0,\"lowered_stop_freq_rare_removed\"].split())\n",
        "# print(pos_tagged_text)\n",
        "\n",
        "# print(train_df[\"lowered_stop_freq_rare_removed\"].head(5)) #before\n",
        "\n",
        "# def do_lemmatizing_with_POS(in_str):\n",
        "#     new_str=''\n",
        "#     for word in in_str.split():\n",
        "#         tag=nltk.pos_tag(word)[0][1][0]\n",
        "#         new_str=new_str + lem.lemmatize(word, wordnet_map.get(tag,wordnet.NOUN)) + \" \"\n",
        "#     return new_str\n",
        "\n",
        "# train_df[\"Lemmatized\"]=train_df[\"lowered_stop_freq_rare_removed\"].apply(lambda x: do_lemmatizing_with_POS(x))\n",
        "# print(train_df[\"Lemmatized\"].head(5)) #after\n",
        "# train_df[\"Lemmatized\"].isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0ad7a29",
      "metadata": {
        "papermill": {
          "duration": 0.042824,
          "end_time": "2022-03-12T06:11:31.522629",
          "exception": false,
          "start_time": "2022-03-12T06:11:31.479805",
          "status": "completed"
        },
        "tags": [],
        "id": "a0ad7a29"
      },
      "source": [
        "> **Ignoring emojis...**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6cd16c88",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-12T06:11:31.654908Z",
          "iopub.status.busy": "2022-03-12T06:11:31.653595Z",
          "iopub.status.idle": "2022-03-12T06:11:31.678529Z",
          "shell.execute_reply": "2022-03-12T06:11:31.677904Z",
          "shell.execute_reply.started": "2022-03-12T05:43:07.318303Z"
        },
        "papermill": {
          "duration": 0.112196,
          "end_time": "2022-03-12T06:11:31.678700",
          "exception": false,
          "start_time": "2022-03-12T06:11:31.566504",
          "status": "completed"
        },
        "tags": [],
        "id": "6cd16c88"
      },
      "outputs": [],
      "source": [
        "# Removing URLs\n",
        "def remove_urls(text):\n",
        "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    return url_pattern.sub(r'', text)\n",
        "def remove_html(in_str):\n",
        "    html_pattern = re.compile('<.*?>')\n",
        "    return html_pattern.sub(r'', in_str)\n",
        "\n",
        "train_df[\"urls_removed\"]=train_df[\"Lemmatized\"].apply(lambda x: remove_urls(x))\n",
        "train_df[\"html_removed\"]=train_df[\"urls_removed\"].apply(lambda x: remove_html(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2395a7da",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-12T06:11:31.800276Z",
          "iopub.status.busy": "2022-03-12T06:11:31.790018Z",
          "iopub.status.idle": "2022-03-12T06:11:31.834051Z",
          "shell.execute_reply": "2022-03-12T06:11:31.833431Z",
          "shell.execute_reply.started": "2022-03-12T05:43:07.421011Z"
        },
        "papermill": {
          "duration": 0.112646,
          "end_time": "2022-03-12T06:11:31.834214",
          "exception": false,
          "start_time": "2022-03-12T06:11:31.721568",
          "status": "completed"
        },
        "tags": [],
        "id": "2395a7da"
      },
      "outputs": [],
      "source": [
        "# Converting chat words to actual text\n",
        "chat_words_str = \"\"\"\n",
        "AFAIK=As Far As I Know\n",
        "AFK=Away From Keyboard\n",
        "ASAP=As Soon As Possible\n",
        "ATK=At The Keyboard\n",
        "ATM=At The Moment\n",
        "A3=Anytime, Anywhere, Anyplace\n",
        "BAK=Back At Keyboard\n",
        "BBL=Be Back Later\n",
        "BBS=Be Back Soon\n",
        "BFN=Bye For Now\n",
        "B4N=Bye For Now\n",
        "BRB=Be Right Back\n",
        "BRT=Be Right There\n",
        "BTW=By The Way\n",
        "B4=Before\n",
        "B4N=Bye For Now\n",
        "CU=See You\n",
        "CUL8R=See You Later\n",
        "CYA=See You\n",
        "FAQ=Frequently Asked Questions\n",
        "FC=Fingers Crossed\n",
        "FWIW=For What It's Worth\n",
        "FYI=For Your Information\n",
        "GAL=Get A Life\n",
        "GG=Good Game\n",
        "GN=Good Night\n",
        "GMTA=Great Minds Think Alike\n",
        "GR8=Great!\n",
        "G9=Genius\n",
        "IC=I See\n",
        "ICQ=I Seek you (also a chat program)\n",
        "ILU=ILU: I Love You\n",
        "IMHO=In My Honest/Humble Opinion\n",
        "IMO=In My Opinion\n",
        "IOW=In Other Words\n",
        "IRL=In Real Life\n",
        "KISS=Keep It Simple, Stupid\n",
        "LDR=Long Distance Relationship\n",
        "LMAO=Laugh My A.. Off\n",
        "LOL=Laughing Out Loud\n",
        "LTNS=Long Time No See\n",
        "L8R=Later\n",
        "MTE=My Thoughts Exactly\n",
        "M8=Mate\n",
        "NRN=No Reply Necessary\n",
        "OIC=Oh I See\n",
        "PITA=Pain In The A..\n",
        "PRT=Party\n",
        "PRW=Parents Are Watching\n",
        "ROFL=Rolling On The Floor Laughing\n",
        "ROFLOL=Rolling On The Floor Laughing Out Loud\n",
        "ROTFLMAO=Rolling On The Floor Laughing My A.. Off\n",
        "SK8=Skate\n",
        "STATS=Your sex and age\n",
        "ASL=Age, Sex, Location\n",
        "THX=Thank You\n",
        "TTFN=Ta-Ta For Now!\n",
        "TTYL=Talk To You Later\n",
        "U=You\n",
        "U2=You Too\n",
        "U4E=Yours For Ever\n",
        "WB=Welcome Back\n",
        "WTF=What The F...\n",
        "WTG=Way To Go!\n",
        "WUF=Where Are You From?\n",
        "W8=Wait...\n",
        "7K=Sick:-D Laugher\n",
        "\"\"\"\n",
        "\n",
        "chat_words_expanded_dict = {}\n",
        "chat_words_list = []\n",
        "for line in chat_words_str.split(\"\\n\"):\n",
        "    if line != \"\":\n",
        "        chat_word = line.split(\"=\")[0]\n",
        "        chat_word_expanded = line.split(\"=\")[1]\n",
        "        chat_words_list.append(chat_word)\n",
        "        chat_words_expanded_dict[chat_word] = chat_word_expanded\n",
        "chat_words_list = set(chat_words_list)\n",
        "\n",
        "def convert_chat_words(in_str):\n",
        "    new_str = \"\"\n",
        "    for w in in_str.split():\n",
        "        if w.upper() in chat_words_list:\n",
        "            new_str = new_str + chat_words_expanded_dict[w.upper()] + \" \"\n",
        "        else:\n",
        "            new_str = new_str + w + \" \"\n",
        "    return new_str\n",
        "\n",
        "train_df[\"chat_words_coverted\"]=train_df[\"html_removed\"].apply(lambda x: convert_chat_words(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc988969",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-12T06:11:31.926907Z",
          "iopub.status.busy": "2022-03-12T06:11:31.925701Z",
          "iopub.status.idle": "2022-03-12T06:11:44.061603Z",
          "shell.execute_reply": "2022-03-12T06:11:44.060991Z",
          "shell.execute_reply.started": "2022-03-12T05:43:07.512523Z"
        },
        "papermill": {
          "duration": 12.184518,
          "end_time": "2022-03-12T06:11:44.061774",
          "exception": false,
          "start_time": "2022-03-12T06:11:31.877256",
          "status": "completed"
        },
        "tags": [],
        "id": "bc988969",
        "outputId": "00edadde-eace-40b9-b61e-10914cfa1f12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pyspellchecker\r\n",
            "  Downloading pyspellchecker-0.6.3-py3-none-any.whl (2.7 MB)\r\n",
            "     |████████████████████████████████| 2.7 MB 625 kB/s            \r\n",
            "\u001b[?25hInstalling collected packages: pyspellchecker\r\n",
            "Successfully installed pyspellchecker-0.6.3\r\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\r\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspellchecker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74358015",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-12T06:11:44.168412Z",
          "iopub.status.busy": "2022-03-12T06:11:44.167342Z",
          "iopub.status.idle": "2022-03-12T09:44:25.272979Z",
          "shell.execute_reply": "2022-03-12T09:44:25.271991Z",
          "shell.execute_reply.started": "2022-03-12T05:43:16.321886Z"
        },
        "papermill": {
          "duration": 12761.162279,
          "end_time": "2022-03-12T09:44:25.273191",
          "exception": false,
          "start_time": "2022-03-12T06:11:44.110912",
          "status": "completed"
        },
        "tags": [],
        "id": "74358015"
      },
      "outputs": [],
      "source": [
        "# Spelling Correction\n",
        "\n",
        "from spellchecker import SpellChecker\n",
        "\n",
        "spell = SpellChecker()\n",
        "\n",
        "def correct_spellings(in_str):\n",
        "    new_str = \"\"\n",
        "    misspelled_words = spell.unknown(in_str.split())\n",
        "    for word in in_str.split():\n",
        "        if word in misspelled_words:\n",
        "            new_str = new_str + spell.correction(word) + \" \"\n",
        "        else:\n",
        "            new_str = new_str + word + \" \"\n",
        "    return new_str\n",
        "\n",
        "train_df[\"spellings_corrected\"]=train_df[\"chat_words_coverted\"].apply(lambda x: correct_spellings(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3656fed",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-12T09:44:25.375742Z",
          "iopub.status.busy": "2022-03-12T09:44:25.374598Z",
          "iopub.status.idle": "2022-03-12T09:44:25.377790Z",
          "shell.execute_reply": "2022-03-12T09:44:25.377133Z",
          "shell.execute_reply.started": "2022-03-12T05:43:16.330635Z"
        },
        "papermill": {
          "duration": 0.056996,
          "end_time": "2022-03-12T09:44:25.377935",
          "exception": false,
          "start_time": "2022-03-12T09:44:25.320939",
          "status": "completed"
        },
        "tags": [],
        "id": "f3656fed"
      },
      "outputs": [],
      "source": [
        "# train_df[\"spellings_corrected\"]=train_df[\"chat_words_coverted\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b30bdde8",
      "metadata": {
        "papermill": {
          "duration": 0.048683,
          "end_time": "2022-03-12T09:44:25.476982",
          "exception": false,
          "start_time": "2022-03-12T09:44:25.428299",
          "status": "completed"
        },
        "tags": [],
        "id": "b30bdde8"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08ded457",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-12T09:44:25.683764Z",
          "iopub.status.busy": "2022-03-12T09:44:25.682569Z",
          "iopub.status.idle": "2022-03-12T09:44:25.687949Z",
          "shell.execute_reply": "2022-03-12T09:44:25.686884Z",
          "shell.execute_reply.started": "2022-03-12T05:43:16.339949Z"
        },
        "papermill": {
          "duration": 0.126173,
          "end_time": "2022-03-12T09:44:25.688255",
          "exception": false,
          "start_time": "2022-03-12T09:44:25.562082",
          "status": "completed"
        },
        "tags": [],
        "id": "08ded457",
        "outputId": "3fca87ea-161b-4c58-955c-02e1fdf10648"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(3263, 4)\n",
            "(10876, 13)\n",
            "(7613, 5)\n"
          ]
        }
      ],
      "source": [
        "print(test_df.shape)\n",
        "print(train_df.shape)\n",
        "print(train_df_copy.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a0c6278",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-12T09:44:25.882159Z",
          "iopub.status.busy": "2022-03-12T09:44:25.881217Z",
          "iopub.status.idle": "2022-03-12T09:44:25.934255Z",
          "shell.execute_reply": "2022-03-12T09:44:25.936108Z",
          "shell.execute_reply.started": "2022-03-12T05:43:16.349867Z"
        },
        "papermill": {
          "duration": 0.149768,
          "end_time": "2022-03-12T09:44:25.936402",
          "exception": false,
          "start_time": "2022-03-12T09:44:25.786634",
          "status": "completed"
        },
        "tags": [],
        "id": "2a0c6278",
        "outputId": "9cd7f9c6-160e-4b6f-e4b2-d17d5c895ef1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(3263, 13)\n",
            "(7613, 13)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  import sys\n",
            "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n",
            "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  del sys.path[0]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>lowered_text</th>\n",
              "      <th>lowered_text_stop_removed</th>\n",
              "      <th>lowered_text_stop_removed_freq_removed</th>\n",
              "      <th>lowered_stop_freq_rare_removed</th>\n",
              "      <th>Lemmatized</th>\n",
              "      <th>urls_removed</th>\n",
              "      <th>html_removed</th>\n",
              "      <th>chat_words_coverted</th>\n",
              "      <th>spellings_corrected</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>deed reason earthquake may allah forgive You</td>\n",
              "      <td>our deeds are the reason of this earthquake ma...</td>\n",
              "      <td>deeds reason earthquake may allah forgive us</td>\n",
              "      <td>deeds reason earthquake may allah forgive us</td>\n",
              "      <td>deeds reason earthquake may allah forgive us</td>\n",
              "      <td>deed reason earthquake may allah forgive u</td>\n",
              "      <td>deed reason earthquake may allah forgive u</td>\n",
              "      <td>deed reason earthquake may allah forgive u</td>\n",
              "      <td>deed reason earthquake may allah forgive You</td>\n",
              "      <td>deed reason earthquake may allah forgive You</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>forest near la range ask canada</td>\n",
              "      <td>forest fire near la ronge sask canada</td>\n",
              "      <td>forest fire near la ronge sask canada</td>\n",
              "      <td>forest near la ronge sask canada</td>\n",
              "      <td>forest near la ronge sask canada</td>\n",
              "      <td>forest near la ronge sask canada</td>\n",
              "      <td>forest near la ronge sask canada</td>\n",
              "      <td>forest near la ronge sask canada</td>\n",
              "      <td>forest near la ronge sask canada</td>\n",
              "      <td>forest near la range ask canada</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>resident asked shelter place notified officer ...</td>\n",
              "      <td>all residents asked to shelter in place are be...</td>\n",
              "      <td>residents asked shelter place notified officer...</td>\n",
              "      <td>residents asked shelter place notified officer...</td>\n",
              "      <td>residents asked shelter place notified officer...</td>\n",
              "      <td>resident asked shelter place notified officer ...</td>\n",
              "      <td>resident asked shelter place notified officer ...</td>\n",
              "      <td>resident asked shelter place notified officer ...</td>\n",
              "      <td>resident asked shelter place notified officer ...</td>\n",
              "      <td>resident asked shelter place notified officer ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13000 receive wildfire evacuation order califo...</td>\n",
              "      <td>13000 people receive wildfires evacuation orde...</td>\n",
              "      <td>13000 people receive wildfires evacuation orde...</td>\n",
              "      <td>13000 receive wildfires evacuation orders cali...</td>\n",
              "      <td>13000 receive wildfires evacuation orders cali...</td>\n",
              "      <td>13000 receive wildfire evacuation order califo...</td>\n",
              "      <td>13000 receive wildfire evacuation order califo...</td>\n",
              "      <td>13000 receive wildfire evacuation order califo...</td>\n",
              "      <td>13000 receive wildfire evacuation order califo...</td>\n",
              "      <td>13000 receive wildfire evacuation order califo...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>got sent photo ruby alaska smoke wildfire pour...</td>\n",
              "      <td>just got sent this photo from ruby alaska as s...</td>\n",
              "      <td>got sent photo ruby alaska smoke wildfires pou...</td>\n",
              "      <td>got sent photo ruby alaska smoke wildfires pou...</td>\n",
              "      <td>got sent photo ruby alaska smoke wildfires pou...</td>\n",
              "      <td>got sent photo ruby alaska smoke wildfire pour...</td>\n",
              "      <td>got sent photo ruby alaska smoke wildfire pour...</td>\n",
              "      <td>got sent photo ruby alaska smoke wildfire pour...</td>\n",
              "      <td>got sent photo ruby alaska smoke wildfire pour...</td>\n",
              "      <td>got sent photo ruby alaska smoke wildfire pour...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id keyword location                                               text  \\\n",
              "0   1     NaN      NaN      deed reason earthquake may allah forgive You    \n",
              "1   4     NaN      NaN                   forest near la range ask canada    \n",
              "2   5     NaN      NaN  resident asked shelter place notified officer ...   \n",
              "3   6     NaN      NaN  13000 receive wildfire evacuation order califo...   \n",
              "4   7     NaN      NaN  got sent photo ruby alaska smoke wildfire pour...   \n",
              "\n",
              "                                        lowered_text  \\\n",
              "0  our deeds are the reason of this earthquake ma...   \n",
              "1              forest fire near la ronge sask canada   \n",
              "2  all residents asked to shelter in place are be...   \n",
              "3  13000 people receive wildfires evacuation orde...   \n",
              "4  just got sent this photo from ruby alaska as s...   \n",
              "\n",
              "                           lowered_text_stop_removed  \\\n",
              "0      deeds reason earthquake may allah forgive us    \n",
              "1             forest fire near la ronge sask canada    \n",
              "2  residents asked shelter place notified officer...   \n",
              "3  13000 people receive wildfires evacuation orde...   \n",
              "4  got sent photo ruby alaska smoke wildfires pou...   \n",
              "\n",
              "              lowered_text_stop_removed_freq_removed  \\\n",
              "0      deeds reason earthquake may allah forgive us    \n",
              "1                  forest near la ronge sask canada    \n",
              "2  residents asked shelter place notified officer...   \n",
              "3  13000 receive wildfires evacuation orders cali...   \n",
              "4  got sent photo ruby alaska smoke wildfires pou...   \n",
              "\n",
              "                      lowered_stop_freq_rare_removed  \\\n",
              "0      deeds reason earthquake may allah forgive us    \n",
              "1                  forest near la ronge sask canada    \n",
              "2  residents asked shelter place notified officer...   \n",
              "3  13000 receive wildfires evacuation orders cali...   \n",
              "4  got sent photo ruby alaska smoke wildfires pou...   \n",
              "\n",
              "                                          Lemmatized  \\\n",
              "0        deed reason earthquake may allah forgive u    \n",
              "1                  forest near la ronge sask canada    \n",
              "2  resident asked shelter place notified officer ...   \n",
              "3  13000 receive wildfire evacuation order califo...   \n",
              "4  got sent photo ruby alaska smoke wildfire pour...   \n",
              "\n",
              "                                        urls_removed  \\\n",
              "0        deed reason earthquake may allah forgive u    \n",
              "1                  forest near la ronge sask canada    \n",
              "2  resident asked shelter place notified officer ...   \n",
              "3  13000 receive wildfire evacuation order califo...   \n",
              "4  got sent photo ruby alaska smoke wildfire pour...   \n",
              "\n",
              "                                        html_removed  \\\n",
              "0        deed reason earthquake may allah forgive u    \n",
              "1                  forest near la ronge sask canada    \n",
              "2  resident asked shelter place notified officer ...   \n",
              "3  13000 receive wildfire evacuation order califo...   \n",
              "4  got sent photo ruby alaska smoke wildfire pour...   \n",
              "\n",
              "                                 chat_words_coverted  \\\n",
              "0      deed reason earthquake may allah forgive You    \n",
              "1                  forest near la ronge sask canada    \n",
              "2  resident asked shelter place notified officer ...   \n",
              "3  13000 receive wildfire evacuation order califo...   \n",
              "4  got sent photo ruby alaska smoke wildfire pour...   \n",
              "\n",
              "                                 spellings_corrected  target  \n",
              "0      deed reason earthquake may allah forgive You        1  \n",
              "1                   forest near la range ask canada        1  \n",
              "2  resident asked shelter place notified officer ...       1  \n",
              "3  13000 receive wildfire evacuation order califo...       1  \n",
              "4  got sent photo ruby alaska smoke wildfire pour...       1  "
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# splitting dataframe into train_df and test_df\n",
        "train_df_copy2 = train_df\n",
        "\n",
        "test_df = train_df.iloc[7613:,:]\n",
        "train_df = train_df.iloc[:7613,:]\n",
        "\n",
        "test_df['text'] = train_df_copy2.iloc[7613:,:]['spellings_corrected']\n",
        "train_df['text'] = train_df_copy2.iloc[:7613,:]['spellings_corrected']\n",
        "\n",
        "print(test_df.shape)\n",
        "print(train_df.shape)\n",
        "\n",
        "train_df['target'] = train_df_copy['target'].values\n",
        "# 7613\n",
        "# 3263\n",
        "train_df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96292239",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-12T09:44:26.121727Z",
          "iopub.status.busy": "2022-03-12T09:44:26.120495Z",
          "iopub.status.idle": "2022-03-12T09:44:26.125338Z",
          "shell.execute_reply": "2022-03-12T09:44:26.122610Z",
          "shell.execute_reply.started": "2022-03-12T05:43:16.382627Z"
        },
        "papermill": {
          "duration": 0.100434,
          "end_time": "2022-03-12T09:44:26.125523",
          "exception": false,
          "start_time": "2022-03-12T09:44:26.025089",
          "status": "completed"
        },
        "tags": [],
        "id": "96292239",
        "outputId": "473e0be0-4f1a-483c-9a9d-6b0cf218db3e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(3263, 13)\n",
            "(7613, 14)\n"
          ]
        }
      ],
      "source": [
        "print(test_df.shape)\n",
        "print(train_df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9986a98b",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-12T09:44:26.250964Z",
          "iopub.status.busy": "2022-03-12T09:44:26.249973Z",
          "iopub.status.idle": "2022-03-12T09:44:26.253486Z",
          "shell.execute_reply": "2022-03-12T09:44:26.252925Z",
          "shell.execute_reply.started": "2022-03-12T05:43:16.389793Z"
        },
        "papermill": {
          "duration": 0.055576,
          "end_time": "2022-03-12T09:44:26.253636",
          "exception": false,
          "start_time": "2022-03-12T09:44:26.198060",
          "status": "completed"
        },
        "tags": [],
        "id": "9986a98b"
      },
      "outputs": [],
      "source": [
        "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# v = TfidfVectorizer(decode_error='replace', encoding='utf-8')\n",
        "# train_df['text'] = v.fit_transform(train_df['text'].values.astype('U'))\n",
        "# test_df['text'] = v.fit_transform(test_df['text'].values.astype('U'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2f40088",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-12T09:44:26.358796Z",
          "iopub.status.busy": "2022-03-12T09:44:26.357783Z",
          "iopub.status.idle": "2022-03-12T09:44:26.362185Z",
          "shell.execute_reply": "2022-03-12T09:44:26.362688Z",
          "shell.execute_reply.started": "2022-03-12T05:43:16.39688Z"
        },
        "papermill": {
          "duration": 0.059908,
          "end_time": "2022-03-12T09:44:26.362857",
          "exception": false,
          "start_time": "2022-03-12T09:44:26.302949",
          "status": "completed"
        },
        "tags": [],
        "id": "a2f40088",
        "outputId": "470d9541-6293-4b09-9519-1fdc2fbd6ac8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(3263, 13)\n",
            "(7613, 14)\n"
          ]
        }
      ],
      "source": [
        "print(test_df.shape)\n",
        "print(train_df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1a70ed0",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-12T09:44:26.513971Z",
          "iopub.status.busy": "2022-03-12T09:44:26.511851Z",
          "iopub.status.idle": "2022-03-12T09:44:26.548616Z",
          "shell.execute_reply": "2022-03-12T09:44:26.547674Z",
          "shell.execute_reply.started": "2022-03-12T05:43:16.405991Z"
        },
        "papermill": {
          "duration": 0.134875,
          "end_time": "2022-03-12T09:44:26.549021",
          "exception": false,
          "start_time": "2022-03-12T09:44:26.414146",
          "status": "completed"
        },
        "tags": [],
        "id": "c1a70ed0",
        "outputId": "6b11c6bc-45b8-46ad-a1fe-26fabebb36ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   id keyword location                                               text  \\\n",
            "0   0     NaN      NaN                       happened terrible car crash    \n",
            "1   2     NaN      NaN  heard earthquake different city stay safe ever...   \n",
            "2   3     NaN      NaN  forest spot pond goose fleeing across street c...   \n",
            "3   9     NaN      NaN              apocalypse lighting spokane wildfire    \n",
            "4  11     NaN      NaN             typhoon soudelor kill 28 china taiwan    \n",
            "\n",
            "                                        lowered_text  \\\n",
            "0                 just happened a terrible car crash   \n",
            "1  heard about earthquake is different cities sta...   \n",
            "2  there is a forest fire at spot pond geese are ...   \n",
            "3              apocalypse lighting spokane wildfires   \n",
            "4      typhoon soudelor kills 28 in china and taiwan   \n",
            "\n",
            "                           lowered_text_stop_removed  \\\n",
            "0                       happened terrible car crash    \n",
            "1  heard earthquake different cities stay safe ev...   \n",
            "2  forest fire spot pond geese fleeing across str...   \n",
            "3             apocalypse lighting spokane wildfires    \n",
            "4            typhoon soudelor kills 28 china taiwan    \n",
            "\n",
            "              lowered_text_stop_removed_freq_removed  \\\n",
            "0                       happened terrible car crash    \n",
            "1  heard earthquake different cities stay safe ev...   \n",
            "2  forest spot pond geese fleeing across street c...   \n",
            "3             apocalypse lighting spokane wildfires    \n",
            "4            typhoon soudelor kills 28 china taiwan    \n",
            "\n",
            "                      lowered_stop_freq_rare_removed  \\\n",
            "0                       happened terrible car crash    \n",
            "1  heard earthquake different cities stay safe ev...   \n",
            "2  forest spot pond geese fleeing across street c...   \n",
            "3             apocalypse lighting spokane wildfires    \n",
            "4            typhoon soudelor kills 28 china taiwan    \n",
            "\n",
            "                                          Lemmatized  \\\n",
            "0                       happened terrible car crash    \n",
            "1  heard earthquake different city stay safe ever...   \n",
            "2  forest spot pond goose fleeing across street c...   \n",
            "3              apocalypse lighting spokane wildfire    \n",
            "4             typhoon soudelor kill 28 china taiwan    \n",
            "\n",
            "                                        urls_removed  \\\n",
            "0                       happened terrible car crash    \n",
            "1  heard earthquake different city stay safe ever...   \n",
            "2  forest spot pond goose fleeing across street c...   \n",
            "3              apocalypse lighting spokane wildfire    \n",
            "4             typhoon soudelor kill 28 china taiwan    \n",
            "\n",
            "                                        html_removed  \\\n",
            "0                       happened terrible car crash    \n",
            "1  heard earthquake different city stay safe ever...   \n",
            "2  forest spot pond goose fleeing across street c...   \n",
            "3              apocalypse lighting spokane wildfire    \n",
            "4             typhoon soudelor kill 28 china taiwan    \n",
            "\n",
            "                                 chat_words_coverted  \\\n",
            "0                       happened terrible car crash    \n",
            "1  heard earthquake different city stay safe ever...   \n",
            "2  forest spot pond goose fleeing across street c...   \n",
            "3              apocalypse lighting spokane wildfire    \n",
            "4             typhoon soudelor kill 28 china taiwan    \n",
            "\n",
            "                                 spellings_corrected  \n",
            "0                       happened terrible car crash   \n",
            "1  heard earthquake different city stay safe ever...  \n",
            "2  forest spot pond goose fleeing across street c...  \n",
            "3              apocalypse lighting spokane wildfire   \n",
            "4             typhoon soudelor kill 28 china taiwan   \n",
            "   id keyword location                                               text  \\\n",
            "0   1     NaN      NaN      deed reason earthquake may allah forgive You    \n",
            "1   4     NaN      NaN                   forest near la range ask canada    \n",
            "2   5     NaN      NaN  resident asked shelter place notified officer ...   \n",
            "3   6     NaN      NaN  13000 receive wildfire evacuation order califo...   \n",
            "4   7     NaN      NaN  got sent photo ruby alaska smoke wildfire pour...   \n",
            "\n",
            "                                        lowered_text  \\\n",
            "0  our deeds are the reason of this earthquake ma...   \n",
            "1              forest fire near la ronge sask canada   \n",
            "2  all residents asked to shelter in place are be...   \n",
            "3  13000 people receive wildfires evacuation orde...   \n",
            "4  just got sent this photo from ruby alaska as s...   \n",
            "\n",
            "                           lowered_text_stop_removed  \\\n",
            "0      deeds reason earthquake may allah forgive us    \n",
            "1             forest fire near la ronge sask canada    \n",
            "2  residents asked shelter place notified officer...   \n",
            "3  13000 people receive wildfires evacuation orde...   \n",
            "4  got sent photo ruby alaska smoke wildfires pou...   \n",
            "\n",
            "              lowered_text_stop_removed_freq_removed  \\\n",
            "0      deeds reason earthquake may allah forgive us    \n",
            "1                  forest near la ronge sask canada    \n",
            "2  residents asked shelter place notified officer...   \n",
            "3  13000 receive wildfires evacuation orders cali...   \n",
            "4  got sent photo ruby alaska smoke wildfires pou...   \n",
            "\n",
            "                      lowered_stop_freq_rare_removed  \\\n",
            "0      deeds reason earthquake may allah forgive us    \n",
            "1                  forest near la ronge sask canada    \n",
            "2  residents asked shelter place notified officer...   \n",
            "3  13000 receive wildfires evacuation orders cali...   \n",
            "4  got sent photo ruby alaska smoke wildfires pou...   \n",
            "\n",
            "                                          Lemmatized  \\\n",
            "0        deed reason earthquake may allah forgive u    \n",
            "1                  forest near la ronge sask canada    \n",
            "2  resident asked shelter place notified officer ...   \n",
            "3  13000 receive wildfire evacuation order califo...   \n",
            "4  got sent photo ruby alaska smoke wildfire pour...   \n",
            "\n",
            "                                        urls_removed  \\\n",
            "0        deed reason earthquake may allah forgive u    \n",
            "1                  forest near la ronge sask canada    \n",
            "2  resident asked shelter place notified officer ...   \n",
            "3  13000 receive wildfire evacuation order califo...   \n",
            "4  got sent photo ruby alaska smoke wildfire pour...   \n",
            "\n",
            "                                        html_removed  \\\n",
            "0        deed reason earthquake may allah forgive u    \n",
            "1                  forest near la ronge sask canada    \n",
            "2  resident asked shelter place notified officer ...   \n",
            "3  13000 receive wildfire evacuation order califo...   \n",
            "4  got sent photo ruby alaska smoke wildfire pour...   \n",
            "\n",
            "                                 chat_words_coverted  \\\n",
            "0      deed reason earthquake may allah forgive You    \n",
            "1                  forest near la ronge sask canada    \n",
            "2  resident asked shelter place notified officer ...   \n",
            "3  13000 receive wildfire evacuation order califo...   \n",
            "4  got sent photo ruby alaska smoke wildfire pour...   \n",
            "\n",
            "                                 spellings_corrected  target  \n",
            "0      deed reason earthquake may allah forgive You        1  \n",
            "1                   forest near la range ask canada        1  \n",
            "2  resident asked shelter place notified officer ...       1  \n",
            "3  13000 receive wildfire evacuation order califo...       1  \n",
            "4  got sent photo ruby alaska smoke wildfire pour...       1  \n"
          ]
        }
      ],
      "source": [
        "print(test_df.head(5))\n",
        "print(train_df.head(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2877f39a",
      "metadata": {
        "papermill": {
          "duration": 0.092108,
          "end_time": "2022-03-12T09:44:26.744842",
          "exception": false,
          "start_time": "2022-03-12T09:44:26.652734",
          "status": "completed"
        },
        "tags": [],
        "id": "2877f39a"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "622e1b8c",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-12T09:44:26.933540Z",
          "iopub.status.busy": "2022-03-12T09:44:26.932298Z",
          "iopub.status.idle": "2022-03-12T09:44:26.935508Z",
          "shell.execute_reply": "2022-03-12T09:44:26.934571Z",
          "shell.execute_reply.started": "2022-03-12T05:43:16.426108Z"
        },
        "papermill": {
          "duration": 0.102367,
          "end_time": "2022-03-12T09:44:26.935754",
          "exception": false,
          "start_time": "2022-03-12T09:44:26.833387",
          "status": "completed"
        },
        "tags": [],
        "id": "622e1b8c"
      },
      "outputs": [],
      "source": [
        "count_vectorizer = feature_extraction.text.CountVectorizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b60bccab",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-12T09:44:27.137246Z",
          "iopub.status.busy": "2022-03-12T09:44:27.136167Z",
          "iopub.status.idle": "2022-03-12T09:44:27.510315Z",
          "shell.execute_reply": "2022-03-12T09:44:27.512297Z",
          "shell.execute_reply.started": "2022-03-12T05:43:16.432201Z"
        },
        "papermill": {
          "duration": 0.483705,
          "end_time": "2022-03-12T09:44:27.512595",
          "exception": false,
          "start_time": "2022-03-12T09:44:27.028890",
          "status": "completed"
        },
        "tags": [],
        "id": "b60bccab"
      },
      "outputs": [],
      "source": [
        "train_vectors = count_vectorizer.fit_transform(train_df[\"text\"])\n",
        "test_vectors = count_vectorizer.transform(test_df[\"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b4e6995",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-12T09:44:27.733396Z",
          "iopub.status.busy": "2022-03-12T09:44:27.729885Z",
          "iopub.status.idle": "2022-03-12T09:44:27.737433Z",
          "shell.execute_reply": "2022-03-12T09:44:27.734535Z",
          "shell.execute_reply.started": "2022-03-12T05:43:16.626925Z"
        },
        "papermill": {
          "duration": 0.121643,
          "end_time": "2022-03-12T09:44:27.737642",
          "exception": false,
          "start_time": "2022-03-12T09:44:27.615999",
          "status": "completed"
        },
        "tags": [],
        "id": "6b4e6995",
        "outputId": "bea30a4a-ec87-413e-b627-edebdc129e12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  (0, 3778)\t1\n",
            "  (0, 15438)\t1\n",
            "  (0, 4447)\t1\n",
            "  (0, 13065)\t1\n",
            "  (0, 904)\t1\n",
            "  (0, 5398)\t1\n",
            "  (0, 19386)\t1\n",
            "  (1, 5389)\t1\n",
            "  (1, 13738)\t1\n",
            "  (1, 12252)\t1\n",
            "  (1, 15352)\t1\n",
            "  (1, 1289)\t1\n",
            "  (1, 2468)\t1\n",
            "  (2, 15682)\t1\n",
            "  (2, 1292)\t1\n",
            "  (2, 16510)\t2\n",
            "  (2, 14755)\t2\n",
            "  (2, 13970)\t1\n",
            "  (2, 14089)\t1\n",
            "  (2, 4821)\t1\n",
            "  (2, 14251)\t1\n",
            "  (2, 4907)\t1\n",
            "  (3, 4821)\t1\n",
            "  (3, 14251)\t1\n",
            "  (3, 118)\t1\n",
            "  :\t:\n",
            "  (7610, 6205)\t1\n",
            "  (7610, 6)\t1\n",
            "  (7610, 18620)\t1\n",
            "  (7610, 11067)\t1\n",
            "  (7611, 2526)\t1\n",
            "  (7611, 14840)\t1\n",
            "  (7611, 15797)\t1\n",
            "  (7611, 11483)\t1\n",
            "  (7611, 12582)\t1\n",
            "  (7611, 16405)\t1\n",
            "  (7611, 14174)\t1\n",
            "  (7611, 11600)\t1\n",
            "  (7611, 17930)\t1\n",
            "  (7611, 1845)\t2\n",
            "  (7611, 3045)\t1\n",
            "  (7611, 14889)\t1\n",
            "  (7611, 17391)\t1\n",
            "  (7612, 19103)\t1\n",
            "  (7612, 2424)\t1\n",
            "  (7612, 6455)\t1\n",
            "  (7612, 12329)\t1\n",
            "  (7612, 13943)\t1\n",
            "  (7612, 587)\t1\n",
            "  (7612, 15394)\t1\n",
            "  (7612, 10981)\t1\n",
            "  (0, 2526)\t1\n",
            "  (0, 3402)\t1\n",
            "  (0, 6133)\t1\n",
            "  (0, 17757)\t1\n",
            "  (1, 2883)\t1\n",
            "  (1, 4030)\t1\n",
            "  (1, 4447)\t1\n",
            "  (1, 4842)\t1\n",
            "  (1, 6244)\t1\n",
            "  (1, 16068)\t1\n",
            "  (1, 17175)\t1\n",
            "  (2, 671)\t1\n",
            "  (2, 2487)\t1\n",
            "  (2, 5282)\t1\n",
            "  (2, 5389)\t1\n",
            "  (2, 14856)\t1\n",
            "  (2, 16190)\t1\n",
            "  (2, 17069)\t1\n",
            "  (2, 17285)\t1\n",
            "  (3, 1131)\t1\n",
            "  (3, 12511)\t1\n",
            "  (3, 17059)\t1\n",
            "  (3, 19103)\t1\n",
            "  (4, 288)\t1\n",
            "  (4, 2802)\t1\n",
            "  :\t:\n",
            "  (3259, 12677)\t1\n",
            "  (3259, 14929)\t1\n",
            "  (3259, 15775)\t1\n",
            "  (3259, 17227)\t1\n",
            "  (3259, 17254)\t1\n",
            "  (3259, 19167)\t1\n",
            "  (3259, 19235)\t1\n",
            "  (3259, 19339)\t1\n",
            "  (3260, 2778)\t1\n",
            "  (3260, 3900)\t1\n",
            "  (3260, 5936)\t1\n",
            "  (3260, 12547)\t1\n",
            "  (3261, 6213)\t1\n",
            "  (3261, 6545)\t1\n",
            "  (3261, 7229)\t1\n",
            "  (3261, 11663)\t1\n",
            "  (3261, 13134)\t1\n",
            "  (3261, 14313)\t1\n",
            "  (3261, 18968)\t1\n",
            "  (3262, 680)\t1\n",
            "  (3262, 2886)\t1\n",
            "  (3262, 4605)\t1\n",
            "  (3262, 13598)\t1\n",
            "  (3262, 14761)\t1\n",
            "  (3262, 19423)\t1\n"
          ]
        }
      ],
      "source": [
        "print(train_vectors)\n",
        "print(test_vectors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe91afd5",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-12T09:44:27.922068Z",
          "iopub.status.busy": "2022-03-12T09:44:27.921123Z",
          "iopub.status.idle": "2022-03-12T09:44:27.936634Z",
          "shell.execute_reply": "2022-03-12T09:44:27.935987Z",
          "shell.execute_reply.started": "2022-03-12T05:43:16.636654Z"
        },
        "papermill": {
          "duration": 0.111364,
          "end_time": "2022-03-12T09:44:27.936846",
          "exception": false,
          "start_time": "2022-03-12T09:44:27.825482",
          "status": "completed"
        },
        "tags": [],
        "id": "fe91afd5"
      },
      "outputs": [],
      "source": [
        "sample_submission = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0a40dd0",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-12T09:44:28.067771Z",
          "iopub.status.busy": "2022-03-12T09:44:28.066979Z",
          "iopub.status.idle": "2022-03-12T09:44:28.562301Z",
          "shell.execute_reply": "2022-03-12T09:44:28.563417Z",
          "shell.execute_reply.started": "2022-03-12T05:43:16.650447Z"
        },
        "papermill": {
          "duration": 0.565518,
          "end_time": "2022-03-12T09:44:28.563796",
          "exception": false,
          "start_time": "2022-03-12T09:44:27.998278",
          "status": "completed"
        },
        "tags": [],
        "id": "e0a40dd0",
        "outputId": "86bfb1df-9644-4dfb-f3e3-354383616e3c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LogisticRegression(random_state=0)"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "clf_lr = LogisticRegression(random_state=0)\n",
        "clf_lr.fit(train_vectors, train_df[\"target\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72a7ebc8",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-12T09:44:28.742406Z",
          "iopub.status.busy": "2022-03-12T09:44:28.741591Z",
          "iopub.status.idle": "2022-03-12T09:44:28.753182Z",
          "shell.execute_reply": "2022-03-12T09:44:28.753743Z",
          "shell.execute_reply.started": "2022-03-12T05:43:17.108254Z"
        },
        "papermill": {
          "duration": 0.072173,
          "end_time": "2022-03-12T09:44:28.753894",
          "exception": false,
          "start_time": "2022-03-12T09:44:28.681721",
          "status": "completed"
        },
        "tags": [],
        "id": "72a7ebc8"
      },
      "outputs": [],
      "source": [
        "sample_submission[\"target\"] = clf_lr.predict(test_vectors)\n",
        "sample_submission.to_csv(\"submission.csv\", index=False)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 12808.921994,
      "end_time": "2022-03-12T09:44:31.552153",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2022-03-12T06:11:02.630159",
      "version": "2.3.3"
    },
    "colab": {
      "name": "IRS_P2_19BCE245.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}