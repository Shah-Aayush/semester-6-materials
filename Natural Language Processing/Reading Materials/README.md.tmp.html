<meta charset="UTF-8">
<h1>NLP Syllabus</h1>

<blockquote>
  <p><em>(topics covered ðŸ˜³)</em></p>
</blockquote>

<ul>
<li>## Introduction to NLP
<ul>
<li>Natural Language Processing, or NLP is a subfield of Artificial Intelligence research that is focused on developing models and points of interaction between humans and computers based on natural language. This includes text, but also speech-based systems.</li>
<li>Computer scientists and researchers have been studying this topic for entire decades, but only recently has it become a hot topic again, a situation made possible by recent breakthroughs in the research community.</li>
<li>The ultimate goal of NLP is to help computers understand language as well as we do. It is the driving force behind things like virtual assistants, speech recognition, sentiment analysis, automatic text summarization, machine translation and much more.</li>
<li>Applications of NLP techniques include voice assistants like Amazon's Alexa and Apple's Siri, but also things like machine translation and text-filtering.
<blockquote>
  <p>How Natural Language Processing works
<ul>
<li><em>A generally accepted truth in computer science is that every complex problem becomes easier to solve if we break it into smaller pieces. That is especially true in the Artificial Intelligence field. For a given problem, we build several small, highly specialized components that are good at solving one and only one problem. We then align all this components, we pass our input through each component and we get our output at the end of the line. This is what we call a pipeline.</em></li>
<li><em>In the NLP context, a basic problem would be that for a given paragraph, the computer understands exactly the meaning of it and then possibly it acts accordingly. For this to work, we need to go through a few steps.</em></li>
</ul></li>
</ul></li>
<li>## Applications of NLP</p>

<h2><ul></h2>

<p></ul></li>
<li>## Tasks of NLP</li>
<li>## NLP Challenges</li>
<li>## Hard and easy problems of NLP</li>
<li><h2>Language Model</h2></p>
</blockquote>

<ul>
<li><a href="./PDFs%20only/Language%20Models.pdf"><strong>refer PDF</strong></a></li>
<li>perplexity
<ul>
<li>The best language model is one that best predicts an unseen test set
<ul>
<li>Gives the highest P(sentence)</li>
</ul></li>
<li>Perplexity is the inverse probability of the test set, normalized by the number of words</li>
<li>PP(W) = P(w<sub>1</sub>,w<sub>2</sub>,...,w<sub>N</sub>)<sup>âˆ’N1</sup></li>
<li>similarly applied from bigrams and chain rules</li>
<li>Minimizing perplexity is the same as maximizing probability</li>
<li>Lower perplexity = better model</li>
</ul></li>
<li><p>The Shannon Visualization Method
<ul>
<li>Choose a random bigram</li>
<li>(<s>, w) according to its probability</li>
<li>Now choose a random bigram (w, x) according to its probability</li>
<li>And so on until we choose </s> Then string the words together</li>
<li><s> I | I want | want to | to eat | eat Chinese | Chinese food | food  </s> | </p>

<blockquote>
  <p>I want to eat Chinese food</li>
</ul></p>
</blockquote></li>
<li><p>The perils <em>(danger)</em> of overfitting</p>

<ul>
<li>N-grams only work well for word prediction if the test corpus looks like the training corpus</li>
<li>In real life, it often doesnâ€™t</li>
<li>We need to train robust models that generalize! </li>
<li>One kind of generalization: Zeros!</li>
<li>Things that donâ€™t ever occur in the training set </li>
<li>But occur in the test set</li>
<li>Zero probability bigrams
<ul>
<li>Bigrams with zero probability</li>
<li>mean that we will assign 0 probability to the test set!</li>
<li>And hence we cannot compute perplexity (canâ€™t divide by 0)!</li>
</ul></li>
</ul></li>
<li>Backoff and Interpolation
<ul>
<li>Sometimes it helps to use less context</li>
<li>Condition on less context for contexts you havenâ€™t learned much about</li>
<li>Backoff:
<ul>
<li>use trigram if you have good evidence, </li>
<li>otherwise bigram,otherwise unigram</li>
</ul></li>
<li>Interpolation:
<ul>
<li>mix unigram,bigram,trigram</li>
<li>Interpolation works better</li>
</ul></li>
</ul></li>
<li>Unknown words: Open versus closed vocabulary tasks
<ul>
<li>If we know all the words in advanced
<ul>
<li>Vocabulary V is fixed</li>
<li>Closed vocabulary task</li>
</ul></li>
<li>Often we donâ€™t know this
<ul>
<li>Out Of Vocabulary = OOV words</li>
<li>Open vocabulary task</li>
</ul></li>
<li>Instead: create an unknown word token <UNK>
<ul>
<li>Training of <UNK> probabilities
<ul>
<li>Create a fixed lexicon L of size V</li>
<li>At text normalization phase, any training word not in L changed to <UNK></li>
<li>Now we train its probabilities like a normal word</li>
</ul></li>
<li>At decoding time
<ul>
<li>If text input: Use UNK probabilities for any word not in training</li>
</ul></li>
</ul></li>
</ul></li>
<li>Huge web-scale n-grams
<ul>
<li>How to deal with, e.g., Google N-gram corpus         </li>
<li>Pruning
<ul>
<li>Only store N-grams with <code>count&gt;threshold</code>.</li>
<li>Remove singletons of higher-order n-grams</li>
<li>Entropy-based pruning</li>
</ul></li>
<li>Efficiency
<ul>
<li>Efficient data structures like tries</li>
<li>Bloom filters:approximate language models        </li>
<li>Store words as indexes,not strings</li>
<li>Use Huffman codingt of it large numbers of words into two bytes </li>
<li>Quantize probabilities(4-8 bits instead of 8-byte float)</li>
</ul></li>
</ul></li>
<li>Stupid backoff
<ul>
<li>no discounting just use relative frequencies</li>
<li><img src="./assets/stupid_backoff.png" alt="" title="" /></li>
</ul></li>
<li>N-gram Smoothing Summary
<ul>
<li>Add-1 smoothing:</li>
<li>OK for text categorization, not for language modeling</li>
<li>The most commonly used method: <br />
<ul>
<li>Extended Interpolated Kneser-Ney</li>
</ul></li>
<li>For very large N-grams like the Web: <br />
<ul>
<li>Stupid backoff</li>
</ul></li>
</ul></li>
<li>Kneser-Ney Smoothing
<ul>
<li><em>refer PDF</em></li>
</ul></li>
</ul></li>
<li><h2>Text processing</h2></li>
<li>## Regular expression</li>
<li>## Regular expression substitution </li>
<li>## A chatbot (Eliza)</li>
<li>## Words and corpora</li>
<li>## Basic Text processing</li>
<li>## Stemming</li>
<li>## stemming Porter stemmer </li>
<li>## POS tagging</li>
<li>## Word representation </li>
<li>## one hot encoding</li>
<li>## Distributional representation</li>
<li><h2>basics of CBOW, GLOVE and Skip gram</h2>

<ul>
<li><a href="./PDFs%20 only/Continuous%20bag%20of%20words%20model.pdf"><strong><em>refer PDF</em></strong></a></li>
<li><h3>Continous Bag Of Words (CBOW)</h3>

<ul>
<li>count based models uses co-occurrence counts of words</li>
<li>(direct) prediction based models directly learn word representations</li>
<li>task : predict n-th word given previous n-1 words</li>
<li>example : he sat on a chair</li>
<li>training data : 
<ul>
<li>all n-word windows in the corpus</li>
<li>training data for this task is easily available (can take all n word from wikipedia)</li>
<li>for ease of illustration, we will first focus on the case when n=2, (i.e. predict second word based on first word)</li>
</ul></li>
<li>we will model this problem using feed forward neural network</li>
<li>input : one hot representation of the context word</li>
<li>output : V words possible and we want to predict probability distribution over V classes</li>
<li><p>some problems : </p>

<ul>
<li>softmax functino at the output is computatioally very expensive</li>
<li>denominator requires a summation over all words in the vocabulary </li>
</ul></li>
<li><p>CBOW : predicts output word / given bag of context words</p></li>
<li>SKIP gram : predicts output contexts / given word</li>
<li>Glove representation : 
<ul>
<li>count based method (SVD) rely on global co-occurrence counts from the corpus for computing word representations</li>
<li>predicts based methods learn word representations using co-occurrence information</li>
</ul></li>
</ul></li>
</ul></li>
<li>## IR model</li>
<li>## Boolean model</li>
<li>## Vector space model</li>
<li>## Word sense disambiguation </li>
<li>## Lesk algorithm for word sense disambiguation </li>
<li>## Concept of lexical analysis,syntax analysis and semantic analysis </li>
<li>## NLP and Deep learning</li>
<li>## Sequence learning problems</li>
<li>## Concept of Selective Read, write and erase </li>
<li>## RNN,LSTM and GRU</li>
<li>## Text classification</li>
<li>## Examples of Text classification</li>
</ul>
