<meta charset="UTF-8">
<h1>NLP Syllabus</h1>

<blockquote>
  <p><em>(topics covered 😳)</em></p>
</blockquote>

<ul>
<li>## Introduction to NLP
<ul>
<li>Natural Language Processing, or NLP is a subfield of Artificial Intelligence research that is focused on developing models and points of interaction between humans and computers based on natural language. This includes text, but also speech-based systems.</li>
<li>Computer scientists and researchers have been studying this topic for entire decades, but only recently has it become a hot topic again, a situation made possible by recent breakthroughs in the research community.</li>
<li>The ultimate goal of NLP is to help computers understand language as well as we do. It is the driving force behind things like virtual assistants, speech recognition, sentiment analysis, automatic text summarization, machine translation and much more.</li>
<li>Applications of NLP techniques include voice assistants like Amazon's Alexa and Apple's Siri, but also things like machine translation and text-filtering.
<blockquote>
  <p>How Natural Language Processing works
<ul>
<li><em>A generally accepted truth in computer science is that every complex problem becomes easier to solve if we break it into smaller pieces. That is especially true in the Artificial Intelligence field. For a given problem, we build several small, highly specialized components that are good at solving one and only one problem. We then align all this components, we pass our input through each component and we get our output at the end of the line. This is what we call a pipeline.</em></li>
<li><em>In the NLP context, a basic problem would be that for a given paragraph, the computer understands exactly the meaning of it and then possibly it acts accordingly. For this to work, we need to go through a few steps.</em></li>
</ul></li>
</ul></li>
<li><h2><a href="https://monkeylearn.com/blog/natural-language-processing-applications/">Applications of NLP</a></h2></p>
</blockquote>

<ul>
<li>Sentiment Analysis</li>
<li>Text Classification</li>
<li>Text Summarization</li>
<li>Text Extraction</li>
<li>Chatbots &amp; Virtual Assistants</li>
<li>Machine Translation</li>
<li>Market Intelligence</li>
<li>Auto-Correct</li>
<li>Intent Classification</li>
<li>Urgency Detection</li>
<li>Speech Recognition</li>
</ul></li>
<li><h2><a href="https://monkeylearn.com/natural-language-processing/#:~:text=Common%20NLP%20Tasks%20%26%20Techniques">Tasks of NLP</a></h2>

<ul>
<li>Syntactic analysis <em>(parsing or syntax analysis)</em></li>
<li>Semantic analysis</li>
<li>Tokenization</li>
<li>Part-of-speech tagging</li>
<li>Dependency Parsing</li>
<li>Constituency Parsing</li>
<li>Lemmatization &amp; Stemming</li>
<li>Stopword Removal</li>
<li>Word Sense Disambiguation</li>
<li>Named Entity Recognition (NER)</li>
</ul></li>
<li><h2><a href="https://www.analyticsinsight.net/10-major-challenges-of-using-natural-language-processing/">NLP Challenges</a></h2>

<ul>
<li>Development Time</li>
<li>Phrasing Ambiguities</li>
<li>Misspellings</li>
<li>Language Differences</li>
<li>Training Data</li>
<li>Innate Biases</li>
<li>Words with Multiple Meanings</li>
<li>Phrases with Multiple Intentions</li>
<li>Keeping a Conversation Moving</li>
</ul></li>
<li><h2>Hard and easy problems of NLP</h2>

<ul>
<li>Hard : 
<ul>
<li>Contextual words and phrases and homonyms.</li>
<li>Synonyms.</li>
<li>Irony and sarcasm.</li>
<li>Ambiguity.</li>
<li>Errors in text or speech.</li>
<li>Colloquialisms and slang.</li>
<li>Domain-specific language.</li>
</ul></li>
<li>Easy : 
<ul>
<li><em>?</em></li>
</ul></li>
</ul></li>
<li><h2>Language Model</h2>

<ul>
<li><a href="./PDFs%20only/Language%20Models.pdf"><strong>refer PDF</strong></a></li>
<li>perplexity
<ul>
<li>The best language model is one that best predicts an unseen test set
<ul>
<li>Gives the highest P(sentence)</li>
</ul></li>
<li>Perplexity is the inverse probability of the test set, normalized by the number of words</li>
<li>PP(W) = P(w<sub>1</sub>,w<sub>2</sub>,...,w<sub>N</sub>)<sup>−N1</sup></li>
<li>similarly applied from bigrams and chain rules</li>
<li>Minimizing perplexity is the same as maximizing probability</li>
<li>Lower perplexity = better model</li>
</ul></li>
<li><p>The Shannon Visualization Method
<ul>
<li>Choose a random bigram</li>
<li>(<s>, w) according to its probability</li>
<li>Now choose a random bigram (w, x) according to its probability</li>
<li>And so on until we choose </s> Then string the words together</li>
<li><s> I | I want | want to | to eat | eat Chinese | Chinese food | food  </s> | </p>

<blockquote>
  <p>I want to eat Chinese food</li>
</ul></p>
</blockquote></li>
<li><p>The perils <em>(danger)</em> of overfitting</p>

<ul>
<li>N-grams only work well for word prediction if the test corpus looks like the training corpus</li>
<li>In real life, it often doesn’t</li>
<li>We need to train robust models that generalize! </li>
<li>One kind of generalization: Zeros!</li>
<li>Things that don’t ever occur in the training set </li>
<li>But occur in the test set</li>
<li>Zero probability bigrams
<ul>
<li>Bigrams with zero probability</li>
<li>mean that we will assign 0 probability to the test set!</li>
<li>And hence we cannot compute perplexity (can’t divide by 0)!</li>
</ul></li>
</ul></li>
<li>Backoff and Interpolation
<ul>
<li>Sometimes it helps to use less context</li>
<li>Condition on less context for contexts you haven’t learned much about</li>
<li>Backoff:
<ul>
<li>use trigram if you have good evidence, </li>
<li>otherwise bigram,otherwise unigram</li>
</ul></li>
<li>Interpolation:
<ul>
<li>mix unigram,bigram,trigram</li>
<li>Interpolation works better</li>
</ul></li>
</ul></li>
<li>Unknown words: Open versus closed vocabulary tasks
<ul>
<li>If we know all the words in advanced
<ul>
<li>Vocabulary V is fixed</li>
<li>Closed vocabulary task</li>
</ul></li>
<li>Often we don’t know this
<ul>
<li>Out Of Vocabulary = OOV words</li>
<li>Open vocabulary task</li>
</ul></li>
<li>Instead: create an unknown word token <UNK>
<ul>
<li>Training of <UNK> probabilities
<ul>
<li>Create a fixed lexicon L of size V</li>
<li>At text normalization phase, any training word not in L changed to <UNK></li>
<li>Now we train its probabilities like a normal word</li>
</ul></li>
<li>At decoding time
<ul>
<li>If text input: Use UNK probabilities for any word not in training</li>
</ul></li>
</ul></li>
</ul></li>
<li>Huge web-scale n-grams
<ul>
<li>How to deal with, e.g., Google N-gram corpus         </li>
<li>Pruning
<ul>
<li>Only store N-grams with <code>count&gt;threshold</code>.</li>
<li>Remove singletons of higher-order n-grams</li>
<li>Entropy-based pruning</li>
</ul></li>
<li>Efficiency
<ul>
<li>Efficient data structures like tries</li>
<li>Bloom filters:approximate language models        </li>
<li>Store words as indexes,not strings</li>
<li>Use Huffman codingt of it large numbers of words into two bytes </li>
<li>Quantize probabilities(4-8 bits instead of 8-byte float)</li>
</ul></li>
</ul></li>
<li>Stupid backoff
<ul>
<li>no discounting just use relative frequencies</li>
<li><img src="./assets/stupid_backoff.png" alt="" title="" /></li>
</ul></li>
<li>N-gram Smoothing Summary
<ul>
<li>Add-1 smoothing:</li>
<li>OK for text categorization, not for language modeling</li>
<li>The most commonly used method: <br />
<ul>
<li>Extended Interpolated Kneser-Ney</li>
</ul></li>
<li>For very large N-grams like the Web: <br />
<ul>
<li>Stupid backoff</li>
</ul></li>
</ul></li>
<li>Kneser-Ney Smoothing
<ul>
<li><em>refer PDF</em></li>
</ul></li>
</ul></li>
<li><h2>Text processing</h2></li>
<li>## Regular expression</li>
<li>## Regular expression substitution </li>
<li>## A chatbot (Eliza)</li>
<li>## Words and corpora</li>
<li>## Basic Text processing</li>
<li>## Stemming</li>
<li><h2>stemming Porter stemmer</h2>

<ul>
<li>A consonant in a word is a letter other than A, E, I, O or U. </li>
<li>If a letter is not a consonant it is a vowel.</li>
<li>These may all be represented by the single form <code>[C]VCVC...[V]</code></li>
<li>where the square brackets denote arbitrary presence of their contents.</li>
<li>Using (VC)<sup>m</sup> to denote VC repeated m times, this may again be written as <a href="VC">C</a><sup>m</sup>[V].</li>
<li>m will be called the measure of any word or word part when represented in this form. The case m = 0 covers the null word. Here are some examples:
<ul>
<li>m=0 : TR, EE, TREE, Y, BY.</li>
<li>m=1 : TROUBLE, OATS, TREES, IVY.</li>
<li>m=2 : TROUBLES, PRIVATE, OATEN, ORRERY.</li>
</ul></li>
<li>The rules for removing a suffix will be given in the form (condition) S1 -> S2
<ul>
<li>This means that if a word ends with the suffix S1, and the stem before S1 satisfies the given condition, S1 is replaced by S2. The condition is usually given in terms of m, e.g.</li>
<li>(m > 1) EMENT -></li>
<li>Here S1 is ‘EMENT’ and S2 is null. This would map REPLACEMENT to REPLAC, since REPLAC is a word part for which m = 2.</li>
</ul></li>
<li>The ‘condition’ part may also contain the following:
<ul>
<li><code>*S</code> : the stem ends with S (and similarly for the other letters).</li>
<li><code>*v*</code> : the stem contains a vowel.</li>
<li><code>*d</code> : the stem ends with a double consonant (e.g. -TT, - SS).</li>
<li><code>*o</code> : : the stem ends cvc, where the second c is not W, X or Y (e.g. -WIL, -HOP).</li>
</ul></li>
<li><p>In a set of rules written beneath each other, only one is obeyed, and this will be the one with the longest matching S1 for the given word. For example, with</p>

<ul>
<li>SSES -> SS</li>
<li>IES -> I</li>
<li>SS -> SS </li>
<li>S -></li>
<li>here the conditions are all null) CARESSES maps to CARESS since SSES is the longest match for S1. Equally CARESS maps to CARESS (S1=‘SS’) and CARES to CARE (S1=‘S’).</li>
</ul></li>
<li><p><strong>STEPS</strong> : </p>

<ul>
<li><strong>STEP 1</strong> : deals with plurals and past participles. The subsequent steps are much more straightforward.
<ul>
<li>1a
<ul>
<li><code>SSES</code> -> <code>SS</code>
<ul>
<li>caresses -> caress</li>
</ul></li>
<li><code>IES</code> -> <code>I</code>
<ul>
<li>ponies -> poni</li>
</ul></li>
<li><code>SS</code> -> <code>SS</code>
<ul>
<li>caress -> caress</li>
</ul></li>
<li><code>S</code> -> ``
<ul>
<li>cats -> cat</li>
</ul></li>
</ul></li>
<li>1b
<ul>
<li>(m>0)<code>EED</code> -> <code>EE</code>
<ul>
<li>feed -> feed</li>
<li>aggreed -> agree</li>
</ul></li>
<li><code>(*v*)</code> <code>ED</code> -> <code>
&lt;ul&gt;
&lt;li&gt;plastered - plaster&lt;/li&gt;
&lt;li&gt;bled -&gt; bled&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;(*v*)&lt;/code&gt; &lt;code&gt;ING&lt;/code&gt; -&gt;</code>
<ul>
<li>motoring -> motor</li>
<li>sing -> sing</li>
</ul></li>
</ul></li>
</ul></li>
<li><strong>STEP 2</strong>
<ul>
<li>(m>0) <code>ATIONAL</code> -> <code>ATE</code>
<ul>
<li>relational -> relate</li>
</ul></li>
<li>(m>0) <code>TIONAL</code> -> <code>TION</code>
<ul>
<li>conditional -> condition</li>
</ul></li>
<li>(m>0) <code>ENCI</code> -> <code>ENCE</code>
<ul>
<li>valenci -> valence</li>
</ul></li>
<li>(m>0) <code>IZER</code> -> <code>IZE</code>
<ul>
<li>digitizer -> digitize</li>
</ul></li>
<li>(m>0) <code>ABLI</code> -> <code>ABLE</code>
<ul>
<li>conformabli -> conformable</li>
</ul></li>
</ul></li>
<li><strong>STEP 3</strong>
<ul>
<li>(m>0) <code>ATIVE</code>, <code>FUL</code>, <code>NESS</code> -> ``
<ul>
<li>formative -> form</li>
<li>hopeful -> hope</li>
<li>goodness -> good</li>
</ul></li>
<li>(m>0) <code>ICATE</code> -> <code>IC</code>
<ul>
<li>triplicate -> triplic</li>
</ul></li>
<li>(m>0) <code>ALIZE</code> -> <code>AL</code>
<ul>
<li>formalize -> formal</li>
</ul></li>
<li>(m>0) <code>ICITI</code> -> <code>IC</code>
<ul>
<li>electriciti -> electric</li>
</ul></li>
<li>(m>0) <code>ICAL</code> -> <code>IC</code>
<ul>
<li>electrical -> electric</li>
</ul></li>
</ul></li>
<li><strong>STEP 4</strong>
<ul>
<li>(m>1) <code>AL</code>, <code>ANCE</code>, <code>ENCE</code>, <code>ER</code>, <code>IC</code>, <code>ABLE</code>, <code>IBLE</code>, <code>ANT</code>, <code>EMENT</code>, <code>MENT</code>, <code>ENT</code>, <code>OU</code>, <code>ISM</code>, <code>ATE</code>, <code>ITI</code> -> ``
<ul>
<li>revival -> reviv</li>
<li>allowance -> allow</li>
<li>inference -> infer</li>
<li>airliner -> airlin</li>
<li>activate -> activ</li>
</ul></li>
</ul></li>
<li><strong>STEP 5</strong> 
<ul>
<li>(m>1) <code>E</code> -> <code>
&lt;ul&gt;
&lt;li&gt;probate -&gt; probat&lt;/li&gt;
&lt;li&gt;rate -&gt; rate&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;(m=1 and not &lt;code&gt;*o&lt;/code&gt;) &lt;code&gt;E&lt;/code&gt; -&gt;</code>
<ul>
<li>cease -> ceas</li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li><h2>POS tagging</h2></li>
<li>## Word representation </li>
<li>## one hot encoding</li>
<li>## Distributional representation</li>
<li><h2>basics of CBOW, GLOVE and Skip gram</h2>

<ul>
<li><a href="./PDFs%20 only/Continuous%20bag%20of%20words%20model.pdf"><strong><em>refer PDF</em></strong></a></li>
<li><h3>Continous Bag Of Words (CBOW)</h3>

<ul>
<li>count based models uses co-occurrence counts of words</li>
<li>(direct) prediction based models directly learn word representations</li>
<li>task : predict n-th word given previous n-1 words</li>
<li>example : he sat on a chair</li>
<li>training data : 
<ul>
<li>all n-word windows in the corpus</li>
<li>training data for this task is easily available (can take all n word from wikipedia)</li>
<li>for ease of illustration, we will first focus on the case when n=2, (i.e. predict second word based on first word)</li>
</ul></li>
<li>we will model this problem using feed forward neural network</li>
<li>input : one hot representation of the context word</li>
<li>output : V words possible and we want to predict probability distribution over V classes</li>
<li><p>some problems : </p>

<ul>
<li>softmax functino at the output is computatioally very expensive</li>
<li>denominator requires a summation over all words in the vocabulary </li>
</ul></li>
<li><p>CBOW : predicts output word / given bag of context words</p></li>
<li>SKIP gram : predicts output contexts / given word</li>
<li>Glove representation : 
<ul>
<li>count based method (SVD) rely on global co-occurrence counts from the corpus for computing word representations</li>
<li>predicts based methods learn word representations using co-occurrence information</li>
</ul></li>
</ul></li>
</ul></li>
<li>## IR model</li>
<li>## Boolean model
<ul>
<li>Simple model based on set theory</li>
<li>Queries and documents specified as boolean expressions        -  precise semantics</li>
<li>E.g.,q=ka ∧ (kb ∨ ¬kc)</li>
<li>Terms are either present or absent. Thus, wij e {0,1}</li>
<li>Drawback of boolean model
<ul>
<li>Expressive power of boolean expressions to capture information need and document semantics inadequate</li>
<li>Retrieval based on binary decision criteria (with no partial match) does not reflect our intuitions behind relevance adequately</li>
</ul></li>
<li>As a result
<ul>
<li>Answer set contains either too few or too many documents in response to a user query</li>
<li>No ranking of documents</li>
</ul></li>
</ul></li>
<li>## Vector space model
    -  Task:
        -  Document collection
        -  Query specifies information need: free text
        -  Relevance judgments: depends upon the weighting scheme for all docs
    -  Word evidence: Bag of words <br />
        -  No ordering information
    -  Represent documents and queries as <br />
        -  Vectors of term-based features
            -  Features: tied to occurrence of terms in collection
    -  Solution 1: Binary features: t=1 if presence, 0 otherwise
        -  Similarity: number of terms in common <br />
            -  Dot product</li>
<li><h2>Word sense disambiguation</h2>

<ul>
<li><p>We understand that words have different meanings based on the context of its usage in the sentence. If we talk about human languages, then they are ambiguous too because many words can be interpreted in multiple ways depending upon the context of their occurrence.</p></li>
<li><p>Word sense disambiguation, in natural language processing (NLP), may be defined as the ability to determine which meaning of word is activated by the use of word in a particular context. Lexical ambiguity, syntactic or semantic, is one of the very first problem that any NLP system faces. Part-of-speech (POS) taggers with high level of accuracy can solve Word’s syntactic ambiguity. On the other hand, the problem of resolving semantic ambiguity is called WSD (word sense disambiguation). Resolving semantic ambiguity is harder than resolving syntactic ambiguity.</p></li>
<li><p>For example, consider the two examples of the distinct sense that exist for the word “bass” −</p>

<ul>
<li><p>I can hear bass sound.</p></li>
<li><p>He likes to eat grilled bass.</p></li>
</ul></li>
<li><p>The occurrence of the word bass clearly denotes the distinct meaning. In first sentence, it means frequency and in second, it means fish. Hence, if it would be disambiguated by WSD then the correct meaning to the above sentences can be assigned as follows −</p>

<ul>
<li><p>I can hear bass/frequency sound.</p></li>
<li><p>He likes to eat grilled bass/fish.</p></li>
</ul></li>
<li><p>Evaluation of WSD</p>

<ul>
<li>The evaluation of WSD requires the following two inputs −</li>
</ul></li>
<li><p>A Dictionary</p>

<ul>
<li>The very first input for evaluation of WSD is dictionary, which is used to specify the senses to be disambiguated.</li>
</ul></li>
<li><p>Test Corpus</p>

<ul>
<li><p>Another input required by WSD is the high-annotated test corpus that has the target or correct-senses. The test corpora can be of two types &minsu;</p></li>
<li><p>Lexical sample − This kind of corpora is used in the system, where it is required to disambiguate a small sample of words.</p></li>
<li><p>All-words − This kind of corpora is used in the system, where it is expected to disambiguate all the words in a piece of running text.</p></li>
</ul></li>
</ul></li>
<li><h2>Lesk algorithm for word sense disambiguation</h2>

<ul>
<li>seminal dictionary based method</li>
<li>The Lesk algorithm is based on the idea that words in a given region of the text will have a similar meaning. In the Simplified Lesk Algorithm, the correct meaning of each word context is found by getting the sense which overlaps the most among the given context and its dictionary meaning.</li>
<li>As the name suggests, for disambiguation, these methods primarily rely on dictionaries, treasures and lexical knowledge base. They do not use corpora evidences for disambiguation. The Lesk method is the seminal dictionary-based method introduced by Michael Lesk in 1986. The Lesk definition, on which the Lesk algorithm is based is “measure overlap between sense definitions for all words in context”. However, in 2000, Kilgarriff and Rosensweig gave the simplified Lesk definition as “measure overlap between sense definitions of word and current context”, which further means identify the correct sense for one word at a time. Here the current context is the set of words in surrounding sentence or paragraph.</li>
</ul></li>
<li><h2>Concept of lexical analysis,syntax analysis and semantic analysis</h2>

<ul>
<li>The main difference between syntax analysis and semantic analysis is that syntax analysis takes the tokens generated by the lexical analysis and generates a parse tree while semantic analysis checks whether the parse tree generated by syntax analysis follows the rules of the language</li>
<li><p>syntax analysis</p>

<ul>
<li>Syntactic analysis or parsing or syntax analysis is the third phase of NLP. The purpose of this phase is to draw exact meaning, or you can say dictionary meaning from the text. Syntax analysis checks the text for meaningfulness comparing to the rules of formal grammar. For example, the sentence like “hot ice-cream” would be rejected by semantic analyzer.</li>
</ul>

<p>In this sense, syntactic analysis or parsing may be defined as the process of analyzing the strings of symbols in natural language conforming to the rules of formal grammar. The origin of the word ‘parsing’ is from Latin word ‘pars’ which means ‘part’.</p>

<p>Concept of Parser
It is used to implement the task of parsing. It may be defined as the software component designed for taking input data (text) and giving structural representation of the input after checking for correct syntax as per formal grammar. It also builds a data structure generally in the form of parse tree or abstract syntax tree or other hierarchical structure.</p>

<p>Symbol Table
The main roles of the parse include −</p>

<p>To report any syntax error.</p>

<p>To recover from commonly occurring error so that the processing of the remainder of program can be continued.</p>

<p>To create parse tree.</p>

<p>To create symbol table.</p>

<p><p><p>To produce intermediate representations (IR).</p></li>
</ul></p></li>
<li><h2>NLP and Deep learning</h2></p>

<ul>
<li>In recent years, a variety of deep learning models have been applied to natural language processing (NLP) to improve, accelerate, and automate the text analytics functions and NLP features. Moreover, these models and methods are offering superior solutions to convert unstructured text into valuable data and insights.</li>
<li>The use of neutral networks for NLP did not start until the early 2000s. But by the end of 2010s, neural networks transformed NLP , enhancing or even replacing earlier techniques. This has been made possible because we now have more data to train neural network models and more powerful computing systems to do so.</li>
<li><a href="https://blog.paperspace.com/6-interesting-deep-learning-applications-for-nlp/">Applications</a>
<ul>
<li><ol>
<li>Tokenization and Text Classification</li>
</ol></li>
<li><ol>
<li>Generating Captions for Images</li>
</ol></li>
<li><ol>
<li>Speech Recognition</li>
</ol></li>
<li><ol>
<li>Machine Translation</li>
</ol></li>
<li><ol>
<li>Question Answering (QA)</li>
</ol></li>
<li><ol>
<li>Document Summarization</li>
</ol></li>
</ul></li>
</ul></li>
<li><h2>Sequence learning problems</h2></li>
<li>## Concept of Selective Read, write and erase </li>
<li>## RNN,LSTM and GRU</li>
<li><h2>Text classification</h2>

<h2>- ## Examples of Text classification</h2></li>
<li><h2>List of PDFs</h2>

<ul>
<li>CBOW</li>
<li>Language models</li>
<li>[Hard] NLP Model </li>
</ul></li>
</ul>
