{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Practical2-NLP-19BCE245.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# NLP Practical 2\n",
        "> 19BCE245 - Aayush Shah\n"
      ],
      "metadata": {
        "id": "cE3YKx1L-gdD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Write a program that can fetch email addresses and URLs from the text. (Hint: Using RE module)"
      ],
      "metadata": {
        "id": "4chU7FqbvWUB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f = open(\"input.txt\", \"w\")\n",
        "f.write(\"Hello from 19bce245@nirmauni.ac.in to aayush@shah.com about the meeting @2PM www.google.com www.hello.com/okbye\")\n",
        "f.close()"
      ],
      "metadata": {
        "id": "zIXHLfUeuhAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f = open('input.txt','r')\n",
        "content = f.read()"
      ],
      "metadata": {
        "id": "39aEStVGABsw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CFk6CkWi-Y4R",
        "outputId": "db54bbc7-3733-4fe1-8456-2f93f7e42af9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Email ids : \n",
            "\t> 19bce245@nirmauni.ac.in\n",
            "\t> aayush@shah.com\n",
            "URLs : \n",
            "\t> www.google.com\n",
            "\t> www.hello.com/okbye\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "s = content\n",
        "  \n",
        "# For EMAIL : \n",
        "# \\S matches any non-whitespace character \n",
        "# @ for as in the Email \n",
        "# + for Repeats a character one or more times \n",
        "\n",
        "lst1 = re.findall('\\S+@\\S+', s)\n",
        "\n",
        "# For URL : \n",
        "# \n",
        "\n",
        "lst2 = re.findall('www\\.\\w*?\\.\\w{2,3}\\S*', s)\n",
        "#lst1.extend(lst2)\n",
        "\n",
        "print('Email ids : ')\n",
        "for string in lst1:\n",
        "  print('\\t>',string)\n",
        "\n",
        "print('URLs : ')\n",
        "for string in lst2:\n",
        "  print('\\t>',string)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Write a program to count words from the given input text file. (Hint: Using NLTK module)"
      ],
      "metadata": {
        "id": "vjY9xRdD-o0X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import ngrams, FreqDist\n",
        "sentence = \"this is not a test this is real not a test this is this is real not a test\"\n",
        "data = sentence.split()\n",
        "\n",
        "all_counts = dict()\n",
        "\n",
        "data_analysis = FreqDist(data)\n",
        "\n",
        "filter_words = dict([(m, n) for m, n in data_analysis.items() if len(m) > 0])\n",
        "\n",
        "sum=0\n",
        "print('Word Count : ',filter_words)\n",
        "for i in filter_words:\n",
        "    sum+=filter_words[i]"
      ],
      "metadata": {
        "id": "iMHSDAKU5830",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "792e7c63-8a44-405b-e8c0-8deaa9abfb59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Count :  {'this': 4, 'is': 4, 'not': 3, 'a': 3, 'test': 3, 'real': 2}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Write a program to implement sentence tokenizer using NLTK and spaCy"
      ],
      "metadata": {
        "id": "6ZWAT-Y_-tnM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### a. NLTK implementation : "
      ],
      "metadata": {
        "id": "Z2rFdHq8_Xjp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  import nltk\n",
        "  nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lDNXJabe_HDl",
        "outputId": "458d568e-a976-430c-a0d1-bec1a15df693"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "tokens = nltk.word_tokenize(sentence)\n",
        "print('\\nSentence tokenizer : ',tokens)"
      ],
      "metadata": {
        "id": "xUHYl4ts-w8v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d5dddef-ee9b-4fd8-866c-82c52bade6cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sentence tokenizer :  ['this', 'is', 'not', 'a', 'test', 'this', 'is', 'real', 'not', 'a', 'test', 'this', 'is', 'this', 'is', 'real', 'not', 'a', 'test']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### b. Spacy implementation : "
      ],
      "metadata": {
        "id": "cqQIcrJu_L4a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy \n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"We are learning NLP using spaCy.\") \n",
        "print([token.text for token in doc]) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "asHrnjDS76_z",
        "outputId": "5ab43327-e770-4960-c407-79ad8c6b9dda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['We', 'are', 'learning', 'NLP', 'using', 'spaCy', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### c. Keras implementation : "
      ],
      "metadata": {
        "id": "Q4w_F5mawSsG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "result = text_to_word_sequence(text)\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ab43327-e770-4960-c407-79ad8c6b9dda",
        "id": "lsizqx3pwSsH"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['We', 'are', 'learning', 'NLP', 'using', 'spaCy', '.']\n"
          ]
        }
      ]
    }
  ]
}