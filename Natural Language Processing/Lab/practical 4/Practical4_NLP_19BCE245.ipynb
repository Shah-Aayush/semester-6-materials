{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Practical4-NLP-19BCE245.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# NLP Practical 4\n",
        "> 19BCE245 - Aayush Shah\n",
        "\n",
        "- To Explore and implement Text Preprocessing and Data Representation\n",
        "(Hint : Sentiment analysis using `Word2Vec` and `Glove` encoding.)"
      ],
      "metadata": {
        "id": "Kl1UmCN6w7kL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "C1CyfKzcw4_n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "186731ea-2172-4472-ab17-2138578af9d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ## `Word2Vec` : "
      ],
      "metadata": {
        "id": "qKNTCN-Awkw3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(action = 'ignore')\n",
        "\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Reading 'shakespeare.txt' file\n",
        "sample = open(\"shakespeare.txt\", \"r\")\n",
        "s = sample.read()\n",
        "\n",
        "f = s.replace(\"\\n\", \" \")  # Replacing escape character with space\n",
        "\n",
        "data = []\n",
        "\n",
        "for i in sent_tokenize(f):  # iterating through each sentence in the file\n",
        "\ttemp = []\n",
        "\t\n",
        "\tfor j in word_tokenize(i):  # tokenizing the sentence into words\n",
        "\t\ttemp.append(j.lower())\n",
        "\n",
        "\tdata.append(temp)\n",
        "\n",
        "model1 = gensim.models.Word2Vec(data, min_count = 1,  # Creating CBOW model\n",
        "\t\t\t\t\t\t\tsize = 100, window = 5)\n",
        "\n",
        "print(\"Cosine similarity between 'dog' \" + \"and 'keep' - CBOW : \",\tmodel1.similarity('dog', 'keep')) # Printing results\n",
        "\t\n",
        "print(\"Cosine similarity between 'king' \" +\n",
        "\t\t\t\t\"and 'we' - CBOW : \",\n",
        "\tmodel1.similarity('king', 'we'))\n",
        "\n",
        "model2 = gensim.models.Word2Vec(data, min_count = 1, size = 100,  # Creating Skip Gram model\n",
        "\t\t\t\t\t\t\t\t\t\t\twindow = 5, sg = 1)\n",
        "\n",
        "print(\"Cosine similarity between 'dog' \" + \"and 'keep' - Skip Gram : \", model2.similarity('dog', 'keep')) # Printing results\n",
        "\t\n",
        "print(\"Cosine similarity between 'king' \" + \"and 'we' - Skip Gram : \", model2.similarity('king', 'we'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Em9iTNiUt52e",
        "outputId": "154e1191-d5f7-4095-e81c-19c3b38b7f44"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine similarity between 'dog' and 'keep' - CBOW :  0.3822126\n",
            "Cosine similarity between 'king' and 'we' - CBOW :  0.09798323\n",
            "Cosine similarity between 'dog' and 'keep' - Skip Gram :  0.4914204\n",
            "Cosine similarity between 'king' and 'we' - Skip Gram :  0.38389352\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dae13493"
      },
      "source": [
        "#### One-Hot encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "d1781a71"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import itertools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "81316d9e"
      },
      "outputs": [],
      "source": [
        "docs = [\"I\", \"love\", \"dogs\", \"and\", \"I\", \"desperately\", \"want\", \"one\"]\n",
        "tokens_docs = [doc.split(\" \") for doc in docs]\n",
        "all_tokens = itertools.chain.from_iterable(tokens_docs)\n",
        "word_to_id = {token: idx for idx, token in enumerate(set(all_tokens))}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "9cfe706b"
      },
      "outputs": [],
      "source": [
        "token_ids = [[word_to_id[token] for token in tokens_doc] for tokens_doc in tokens_docs]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "afbbe2b1"
      },
      "outputs": [],
      "source": [
        "vec = OneHotEncoder(categories=\"auto\")\n",
        "X = vec.fit_transform(token_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13dc66f1",
        "outputId": "64eb9467-b56d-4d3f-fae7-8cf59f9b12ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0.]]\n"
          ]
        }
      ],
      "source": [
        "print(X.toarray())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44d3b09d"
      },
      "source": [
        "#### Bag-of-words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "509d94e9"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "74f498bf"
      },
      "outputs": [],
      "source": [
        "text = [\"i love dogs and i want one\"]\n",
        "vectorizer = CountVectorizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "f09b9781",
        "outputId": "6f55ddc7-88cb-4445-a45c-4fdec706bda1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'love': 2, 'dogs': 1, 'and': 0, 'want': 4, 'one': 3}\n"
          ]
        }
      ],
      "source": [
        "vectorizer.fit(text)\n",
        "print(vectorizer.vocabulary_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "64f474ae",
        "outputId": "1cccaa33-f1e8-42d1-cfa0-3d76be5148d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 5)\n",
            "[[1 1 1 1 1]]\n"
          ]
        }
      ],
      "source": [
        "vector = vectorizer.transform(text)\n",
        "print(vector.shape)\n",
        "print(vector.toarray())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16a23f28"
      },
      "source": [
        "#### TF-IDF representation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "0422e8ad"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "d23a45cc",
        "outputId": "ad8f9e25-63dd-4769-891c-9b306055f644",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The text:  ['i love dogs', 'dogs are loyal', 'golden retriver is my favorite dog breed', 'i want a dog desperately but i can do nothing']\n"
          ]
        }
      ],
      "source": [
        "text1 = ['i love dogs', 'dogs are loyal', \n",
        "'golden retriver is my favorite dog breed', \n",
        "'i want a dog desperately but i can do nothing']\n",
        "tf = TfidfVectorizer()\n",
        "txt_fitted = tf.fit(text1)\n",
        "txt_transformed = txt_fitted.transform(text1)\n",
        "print (\"The text: \", text1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "9198f351",
        "outputId": "4ac97879-7405-4e6e-967e-6ea4d806325c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'are': 1.916290731874155, 'breed': 1.916290731874155, 'but': 1.916290731874155, 'can': 1.916290731874155, 'desperately': 1.916290731874155, 'do': 1.916290731874155, 'dog': 1.5108256237659907, 'dogs': 1.5108256237659907, 'favorite': 1.916290731874155, 'golden': 1.916290731874155, 'is': 1.916290731874155, 'love': 1.916290731874155, 'loyal': 1.916290731874155, 'my': 1.916290731874155, 'nothing': 1.916290731874155, 'retriver': 1.916290731874155, 'want': 1.916290731874155}\n"
          ]
        }
      ],
      "source": [
        "idf = tf.idf_\n",
        "print(dict(zip(txt_fitted.get_feature_names(), idf)))"
      ]
    }
  ]
}